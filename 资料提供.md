先看下，web需要掌握的知识点：

HTML
CSS
JavaScript
AJAX
jQuery
Vue.js
下面是Web前端基本的学习路线：

第一阶段：

HTML+CSS/HTML5+CSS3
HTML视频教程主要讲解了HTML基础语法，内容主要包括：
HTML概述、W3C概述、B/S架构系统原理、table、背景色与背景图片、超链接、列表、表单、框架等知识点。
通过该视频的学习之后，可以开发基本的网页，并且可以看懂别人编写的HTML页面。

CSS视频教程主要讲解什么是css 。

层叠样式表(英文全称：CascadingStyleSheets)是一种用来表现HTML（标准通用标记语言的一个应用）或XML（标准通用标记语言的一个子集）等文件样式的计算机语言。

CSS不仅可以静态地修饰网页，还可以配合各种脚本语言动态地对网页各元素进行格式化。

CSS能够对网页中元素位置的排版进行像素级精确控制，支持几乎所有的字体字号样式，拥有对网页对象和模型样式编辑的能力。

适合人群

0基础想入门的同学
基础不牢固的同学
第二阶段：
JavaScript+JS框架技术

主要讲解前端开发中的核心技术JavaScript，俗称JS，视频中讲解了JavaScript核心语法、JavaScript内置支持类、JavaScript调试、JavaScript DOM编程、JavaScript BOM编程、大量前端小案例、JavaScript事件处理、JavaScript对象、继承、JSON等知识点。

适合人群

已具备HTML基础知识与JS基本操作的同学
Web前端爱好者以及在职程序员
第三阶段：Ajax
Ajax技术是基于js语言的扩展，能够通过将请求发送给后台，并从后台取得相关数据，然后将数据在页面做局部刷新的重要技术。

本教程会通过对ajax的传统使用方式，结合json操作的方式，结合跨域等高级技术的方式，对ajax做一个全面的讲解。

适合人群

掌握HTML基本知识和JS基本操作的
第四阶段：jQuery
jQuery是优秀的JavaScript框架，能使用户更方便地处理HTML Documents、events、实现动画效果，并且方便地为网站提供Ajax交互。

本教程循序渐进地对jQuery的各种选择器、函数和方法调用进行了详细的讲解，更结合了大量的案例。

第五阶段：Vue.js
通过视频的学习，让大家掌握Vue.js及在项目中的使用。将一步一步地让大家快速地掌握Vue.js这样一个前端核心框架，以适应公司的开发需要。

Vue (读音 /vju/，类似于 view) 是一套用于构建用户界面的渐进式框架。如果你之前已经习惯了用jQuery操作DOM，学习Vue.js时请先抛开手动操作DOM的思维，因为Vue.js是数据驱动的，你无需手动操作DOM。它通过一些特殊的HTML语法，将DOM和数据绑定起来......

学习路线要配合课程才能学的会的，晚上有时间的朋友可以进学习群听公开课

对于零基础的人而言，要怎么学习Web前端呢？

1、前端页面重构。主要内容为PC端网站布局、Photoshop 工具及切图、H5移动端网页布局、HTML5+CSS3新特性与交互。学习目标是完成PC端网站布局，可实现响应式布局，一套代码适配 PC 端、移动端、平板设备等。

2、前后端网页交互。主要内容为JavaScript语法全面进阶、ES6 到 ES10 新语法实践、jQuery 应用及插件使用、设计模式及插件编写、封装JS工具库及Web APIS、AJAX+PHP+MySQL前后端交互、前端工程化与模块化应用以及PC 端全栈开发项目等。学习目标是可以掌握前端工程化工具，如 git、gulp、Webpack 等，搭建项目及开发项目。

3、Node.js + 前端框架。主要内容为Node.js 全面进阶、Koa2+MongoDB搭建服务、Vue.js 框架、React.js 框架、小程序云开发与小程序框架、原生APP与混合APP、数据可视化与桌面应用等。学习目标是掌握桌面应用及可视化大数据，实现复杂数据展示类项目，能够独立完成前后台相关功能，胜任HTML5全栈开发工程师职位。

最后给要学前端的同学一个忠告：

1、首先，自学是件很苦的事情，学习本身就是件反人性的过程，更何况在一个从零开始的全新领域独自奋斗，这里不是要打击你，而是要你认真想好，结合自身的环境、条件。不然半途而废浪费的不仅是时间，还有精力、金钱。

2、如果你已经想好决定去做，那就不要犹豫，坚持下去你就可以成功，很多学渣都可以，你没有什么不可以的，一定要相信自己。

3、兴趣是最好的老师，很多人对这句话非常反感，被生活所迫，谈兴趣不可笑吗？其实不是，问问自己对技术有没有兴趣，坐不坐得住。做自己喜欢和感兴趣的事情才能够走的更远和更久，很多人不理解这一点，总有一天你会明白的，我当时转前端的时候也是因为自己挺喜欢，而且最接近用户，都是所见所得的东西很好玩，另外一方面也觉得工资也挺高，所以自己打心里觉得做前端挺好。

4、做好持续学习，时刻保持学习的心态，说实话现在社会，科技的发展非常之快，技术的更新更是如此，如果你觉得学习一门技术就可以吃到老，我劝你还是打住，可能考个公务员更适合你（不要杠精，我并不是说公务员不要学习，自己体会就好）。

5、永远不要觉得迟，只要行动然后坚持下去，你就干掉了 80% 的人，自学的人很多，但是坚持下来的没有几个。就像郭德纲说过，不是我的相声说的多好而是我活了下来。

6、与其把时间花在学与不学的纠结上，不如把这个时间放在行动上，如果发现自己不合适也坚持不下去，那就换个赛道，专注的去做一件事情，会提高你的成功概率。



学完HTML、css、JavaScript之后学什么？这张前端学习路线图一定能给你答案！



第一阶段：前端入门网页基础

html5+css3→ 页面布局实战

第二阶段：前端入门开发内功

javascript（基础+进阶）→ ES6→ Ajax→ Promise→Git→node.js→PC端全栈开发实战

第三阶段：前端高级框架技术

vue2+vue3→vue实战项目 →Pinia→React→React实战项目

第四阶段，混合应用开发技术

微信公众号→微信小程序→微信小程序实战项目→ uni-app→uni-app实战项目 react项目

第五阶段：大前端的架构技术

webpack5→Vite2→typescript架构 HarmonyOS app

以前前端开发者只要掌握HTML、CSS、JavaScript 三驾马车就能胜任一份Web前端的工作了。而现在除了普通的编码以外，还要考虑如何性能优化，如何跨端、跨平台实现功能，尤其是 AI、5G 技术的来临，都在加快Web前端技术的更新，也在逼促开发者要不停的学习，不能的接受新的技术标准。

“学什么”“怎么学”其实是我们要着重解决的问题。

这一波良心推荐的前端教程+项目干货，不谈虚的，直接来谈每个阶段要学习的内容以及配套教程！

非常适合新手学习，从最基础的html+css开始讲起，每个视频都是分开的一小节，知识点密集，每个知识点还有实战项目让你练习。

第一阶段：前端入门网页基础
html5+css3→ 页面布局实战（响应式/移动端/pc端页面）

1、 前端入门基础核心必看html+css

这是入门前端最基础的内容。学会后，就能达到初级Web前端工程师水平。熟悉了前端开发的HTML与CSS基础知识，就能够配合UI设计师进行项目布局开发了。

HTML+CSS教程，零基础web前端开发入门必看视频

本系列从概念到具体基础知识点全程干货满满，为前端小白入门找到了很好的学习抓手，可以作为前端开发学习“梦开始的地方”，老师深入浅出的讲解和动画视频解析并用真实的案例巩固知识，学练结合，打好基础，不怕学不会！

2、 页面布局实战

有了第一步的基础知识，你就可以实战各种页面布局了。学会后，更加夯实初级Web前端工程师水平，能够完成各种PC端与移动端网页布局与样式设计实现了。

web前端项目实战之拉勾网（项目上手）

本课程讲的是HTML+css pc端项目实战，通过讲解拉勾网得首页、登录页等，学完即可进一步学会应用HTML+CSS，掌握页面布局和标签、属性等的使用，让你从小白进化为真正的网页美化师。

第二阶段：前端入门开发内功
javascript（基础+进阶）→ ES6→ Ajax→ Promise→Git→node.js→PC端全栈项目开实战

1、javascript（基础+进阶）

JavaScript全套视频教程（10天学会Js，前端javascript入门必备）

本视频主打内容最全最新，包括JS基础，基于面向对象开发实战，前后端交互实战，jQuery与BootStrap，以及CSS预处理器Sass，打造一站式知识长龙服务，适合有HTML和CSS基础的同学学习。

2、ES6

ES6的发布是大前端的里程碑。它的目标是使得JavaScript语言可以用来编写复杂的大型应用程序，成为企业级开发语言 。

最新版Web前端ES6-ES13教程，JavaScript高级进阶视频教程

本视频主打内容最全最新，包括ES6-ES13所有新特性 以及 实战应用，打造一站式知识长龙服务，适合有JS基础的同学学习。

3、 Ajax

Ajax技术可以使网页应用能够快速地将增量更新呈现在用户界面上，而不需要重载（刷新）整个页面。

web前后端交互Ajax从入门到精通全套教程

本视频主打内容最全最新，包括Ajax XHR基础语法、fetch和axios使用、以及 跨域解决方案，打造一站式知识长龙服务，适合有JS基础的同学学习。

4、 Promise

Promise 是异步编程的一种解决方案，比传统的解决方案回调函数, 更合理和更强大。ES6 将其写进了语言标准，统一了用法，原生提供了Promise对象 。

web前端进阶Promise js从入门到实战全套教程

本视频主打内容最全最新，包括Promise基础语法、手写Promise、async与await 以及 实战应用，打造一站式知识长龙服务，适合有JS基础的同学学习。

5、 Git

前端Git教程，从入门到实战迅速上手git

项目管理工具Git教程上线啦～一看就会，上手不废的最新版从入门到实战全套教程，视频包括Git常用命令、Git分支管理、团队以及跨团队协作、VSCode集成Git，稳稳拿捏Git

6、 node.js

前端Node.JS教程,快速入门nodejs全套完整版

本视频你将从认识Nodejs开始学习npm、nrm、yarn；内置模块、路由、express、MongoDB、身份认证、koa、MySQL、Socket、Mocha等知识点全覆盖，学完本系列视频可以让前端程序员插上后端的翅膀，真正成为一名全栈工程师，助力同学横向全面发展。

7、 PC端全栈项目开实战（2选1）

PC端原生JavaScript项目案例实战开发

本PC项目实战视频是基于JS开发，强化ES6-ES13的项目应用，打通前后端数据交互，并实战应用周边生态Sass以及Bootstrap, 打造一站式知识长龙服务，适合有js基础的同学学习。

web前端js+nodejs后端express框架博客系统项目实战教程，前后端交互开发毕设项目

本套使用主要教会大家，如何利用nodejs的一个express框架和js开发一个blog项目。教会大家使用js做后端，使用js做前端，成为一名全栈工程师。

适合作为一个课程设计或者毕业设计，最终我们实现了一个完整的博客系统，包括用户的登录、注册，图片上传，文章的发布、富文本编辑器、删除、编辑、修改、列表展示，评论的发布、删除、列表展示。最终实现了用户的文章和评论的后台管理和博客的前台展示和评论功能。

第三阶段：前端高级框架技术
vue2+vue3→vue实战项目 →Pinia→React→React实战项目

1、 vue2+vue3

Vue2.0+Vue3.0全套教程，vue.js零基础入门到vue项目实战，前端必学框架教程

Vue.js致力于构建数据驱动的web应用开发框架，以简洁化，轻量级，数据驱动，模块友好等优势深受企业以及前端开发者的喜爱，成为前端开发人员必备的技能。 本课程以项目实战为驱动，以轻松幽默的评书演义，帮你打开通往Vue.js的任督二脉，从vue2轻松过渡到vue3,助力同学成为一位优秀的Vue.js开发人员。

2、 vue实战项目

Vue+ElementUI物业后台管理系统

vue项目实战，Vue+ElementUI物业后台管理系统

本项目是利用Vue3.0 + Element Plus UI技术开发后台管理系统，本视频你将见证从零开始搭建项目，手把手教你使用vue3.0组件开发，并用Element UI库快速创建项目页面，使用axios封装与拦截器进行前后端交互，用json-server模拟数据后台创建。在项目中还会讲到工具库的封装、vue环境变量的部署、路由的设置与配置、响应式配置等。

3、 Pinia

前端Pinia教程，Pinia+vue3+vite+ts+腾讯IM聊天解决方案项目实战

基于vue3+vite+ts来学习pinia的使用，并结合当下最流行的聊天解决方案——腾讯IM完成项目实战，并深入封装基于pinia的IM插件。

4、 React

React全家桶教程_react零基础入门到项目实战完整版

React已经成为江湖大厂的主流前端开发框架，本视频基于最新版React17良心制作。对React开发核心技术以及周边技术栈进行详细讲解，并进一步通过实际需求案例驱动知识点吸收，帮助大家迅速成长为React开发高手。最后还有一个完整的后台管理系统项目实战讲解，让大家即使身不入大厂，也可对企业内部真实项目组开发流程与细节做到心中有数。

5、 React实战项目

全球新闻发布管理系统

React项目实战教程（全球新闻发布管理系统）

本项目实战教程一站式应用React全家桶打造企业级后台系统，无缝对接各大门派需求，让你不入江湖，便知江湖风雨。 同学们先去看过上述React开发基础视频，熟悉React 组件开发，组件通信以及基础路由开发再配合食用本实战教程效果更好。

第四阶段，混合应用开发技术
微信公众号→微信小程序→微信小程序实战项目→ uni-app→uni-app实战项目

所谓混合开发，就是将HTML5基于浏览器的应用，嵌入到基于Android和iOS手机APP里，或者嵌入到基于Node和Chromium的桌面APP里。因为兼具了WebApp和NativeApp的双重优点，混合应用开发技术得到了广泛的应用。

学会这个部分，就拥有了多端开发能力，能够胜任跨平台跨设备的架构工作。常见的混合开发如手机端的微信公众号、微信小程序、桌面端的Electron技术和PWA技术等。

1、微信公众号

公众号开发教程全开源（强烈推荐）

本系列课程从整个微信公众号体系的介绍开始，进一步使用编辑模式来完成一个最基本的公众号搭建；此外你还将了解到公众号的开发模式以及所需的服务端相关知识，其中包括Express、MongoDB、Robo3T、mongoose、云服务器等；还会对微信JS-SDK鉴权的整个流程进行梳理与开发；使用vue技术栈结合vant组件库，构建web应用并集成到微信公众平台中。

2、微信小程序

微信小程序开发制作前端教程，零基础轻松入门玩转微信小程序

本系列视频课程分为两大部分，微信小程序基础与项目实战开发。小程序开发基础学习部分你将从开发流程学起，到全局配置、todolist、基础语法、组件及自定义组件的基础知识学习。实战开发部分的讲解则会从项目搭建到借口、封装、首页模块、轮播模块、搜索模块、分类模块、授权模块、购物车及我的模块深度剖析小程序。

3、小程序实战

锋巢直播IM ——基于腾讯云音视频跨平台应用

小程序开发实战项目-锋巢直播IM ——基于腾讯云音视频跨平台应用

本视频使用腾讯即时通信IM+直播电商解决方案组件TLS,并涉及众多腾讯云产品，包括但不限于云直播，云数据库，Serverless，提供了一站式讲解，帮助大家迅速整合直播电商功能到自己的业务中， 通过幽默风趣讲解驱动学习的积极性，让大家不入公司，便知公司项目的全貌。

4、Uniapp

前端uniapp入门到实战项目教程(微信小程序+H5+安卓APP+IOS APP)

前端项目_uni-app入门到实战项目之《仿网易云音乐》

Uniapp 是基于「 Vue + 微信小程序 」语言体系，开发人员学习成本低上手快，随着如今 Uniapp 生态也逐步趋于成熟。基于 Uniapp 开发多端项目，已经是很多中小型企业常用的技术解决方案。 我们就从 Uniapp 基础开始，对照企业级实践标准，从零到一打造一个多端（微信小程序 + H5 + 安卓 app + IOS app）的社区论坛类项目。

5、Uniapp 实战

Vue项目实战，uni-app蛋糕订购项目开发教程（vue+uniapp+小程序）

本项目是一个面向部分城市群体的蛋糕定制网站，为指定区域的用户提供在线定制、实时配送，打造线上线下相结合的个性化蛋糕定制服务。

第五阶段：大前端的架构技术
webpack5→Vite2→typescript

掌握这个部分，即可拥有大前端架构师水平，主要进行前端项目架构和项目把控。能够解决网站出现的突发状况，能够改进网站性能到极致。拥有大型网站、大量高并发访问量等开发经验。

1、webpack5

前端webpack5全套教程，全网最完整的webpack教程（基础+高级）

webpack5课程分为四大部分，分别是webpack基础应用篇，webpack高级应用篇，webpack项目实战篇以及webpack内部原理篇。在本课程中，我们将通过前后呼应的demo从0到1学会webpack5，在项目实战中学以致用并在最后阶段去理解其底层的原理，从而做到对webpack5知其然并知其所以然的精熟掌握程度，完成前端工程师的一大步提升。

2、Vite2

前端Vite学习指南，基于腾讯云的项目教程

Vite 基于原生 ES-Module 推出的前端构建工具，Vite 因为它的跨前端框架的能力 和极其优越的性能，被大家称为下一代前端构建工具，及时的学习新技术是有必要的。本系列视频你将会学到Vite环境的搭建、依赖预构建、模块热重载、在Vite中使用vue2\vue3\CSS等其他技术、服务器渲染等关联技术。

3、typescript

前端 TypeScript 入门教程



前端学习阶段

1 、web前端入门阶段
HTML/HTML5，在实际工作中主要使用的是HTML5，但是HTML作为基础知识也是必须要掌握的。
l CSS/CSS3
l JavaScript（ES6语法）
l 网络请求如Ajax
l JSON、XML类常规数据结构
l 熟悉各种编辑器的使用（目前常用的是：VScode 和 Hbuilder X）
l 学会用Google浏览器（这一点可能很多人觉得自己都会用，其实这里更多的是指如何利用浏览器调试查看自己程序的技能）
l Git,SVN等代码管理工具的使用
2 、WEB前端初级阶段
目前如果你只会HTML、CSS、JavaScript已经远远不能满足市场需求了，你至少得掌握一些框架的运用。
l 至少掌握一个目前流行的三大前端框架（Vue、Angular、React）
l 至少掌握一个UI框架，如Bootstrap，Vant，Element-ui等
l 学会，使用插件和组件，比如应用中常用到的轮播插件Swiper
l 熟练使用JavaScript的ES6语法，他会比ES5方便很多，而且目前兼容性也比较好了
l 学习并使用CSS预处理语言Sass和Less
l 前端模拟请求工具，目前常用的为PostMan
3 、WEB前端中级阶段
l 学习并掌握Node.js的简单开发
l 学习并掌握前端自动化配置 Webpack、Gulp等
l 学习微信小程序的开发
l 数量运用Vue、React、Angular其中之一进行项目开发
l 熟悉Http协议，WebSocket协议的使用
定期关注行业动态，根据市场变化动态补充或者扩展自己的知识储备。
4 、WEB前端高级阶段
l 熟练掌握并运用Web组件化、插件化开发知识
l 掌握服务器知识，比如Node的熟练运用、Nginx的部署等
l 掌握一定的移动端开发技能
l 掌握一定的桌面应用开发技能，如Electron
l 保持一个不断的学习状态，将前端、服务端、桌面程序技能运用自如，同时适当扩展自己研发能力以外的工作能力，如沟通能力、管理能力等。













一、注意力机制：Attention
1.1 什么是注意力机制？
我们先来看一张图片，这个是前几天微博之夜的



那大家的目光更多停留在是在五个美女身上，还是在张大大身上呢 ？（大大老师骚瑞~ORZ）

同样的，不同的粉丝更加关注的对象也是不同的。 

再举几个栗子：

看人-->看脸
看文章-->看标题
看段落-->看开头
这时候大家应该大致知道注意力机制是个什么东西了吧~

注意力机制其实是源自于人对于外部信息的处理能力。由于人每一时刻接受的信息都是无比的庞大且复杂，远远超过人脑的处理能力，因此人在处理信息的时候，会将注意力放在需要关注的信息上，对于其他无关的外部信息进行过滤，这种处理方式被称为注意力机制。

我用通俗的大白话解释一下：注意力呢，对于我们人来说可以理解为“关注度”，对于没有感情的机器来说其实就是赋予多少权重(比如0-1之间的小数)，越重要的地方或者越相关的地方就赋予越高的权重。

1.2 如何运用注意力机制？ 
1.2.1 Query&Key&Value
首先我们来认识几个概念：

查询（Query）： 指的是查询的范围，自主提示，即主观意识的特征向量
键（Key）： 指的是被比对的项，非自主提示，即物体的突出特征信息向量
值（Value） ：  则是代表物体本身的特征向量，通常和Key成对出现
注意力机制是通过Query与Key的注意力汇聚（给定一个 Query，计算Query与 Key的相关性，然后根据Query与Key的相关性去找到最合适的 Value）实现对Value的注意力权重分配，生成最终的输出结果。



有点抽象吧，我们举个栗子好了：

当你用上淘宝购物时，你会敲入一句关键词（比如：显瘦），这个就是Query。
搜索系统会根据关键词这个去查找一系列相关的Key（商品名称、图片）。
最后系统会将相应的 Value （具体的衣服）返回给你。
在这个栗子中，Query， Key 和 Value 的每个属性虽然在不同的空间，其实他们是有一定的潜在关系的，也就是说通过某种变换，可以使得三者的属性在一个相近的空间中。

1.2.2 注意力机制计算过程
输入Query、Key、Value：

阶段一：根据Query和Key计算两者之间的相关性或相似性（常见方法点积、余弦相似度，MLP网络），得到注意力得分；


阶段二：对注意力得分进行缩放scale（除以维度的根号），再softmax函数，一方面可以进行归一化，将原始计算分值整理成所有元素权重之和为1的概率分布；另一方面也可以通过softmax的内在机制更加突出重要元素的权重。一般采用如下公式计算：


阶段三：根据权重系数对Value值进行加权求和，得到Attention Value（此时的V是具有一些注意力信息的，更重要的信息更关注，不重要的信息被忽视了）；


这三个阶段可以用下图表示：


🌟二、自注意力机制：Self-Attention
2.1 什么是自注意力机制？
自注意力机制实际上是注意力机制中的一种，也是一种网络的构型，它想要解决的问题是神经网络接收的输入是很多大小不一的向量，并且不同向量向量之间有一定的关系，但是实际训练的时候无法充分发挥这些输入之间的关系而导致模型训练结果效果极差。比如机器翻译(序列到序列的问题，机器自己决定多少个标签)，词性标注(Pos tagging一个向量对应一个标签)，语义分析(多个向量对应一个标签)等文字处理问题。

针对全连接神经网络对于多个相关的输入无法建立起相关性的这个问题，通过自注意力机制来解决，自注意力机制实际上是想让机器注意到整个输入中不同部分之间的相关性。

自注意力机制是注意力机制的变体，其减少了对外部信息的依赖，更擅长捕捉数据或特征的内部相关性。自注意力机制的关键点在于，Q、K、V是同一个东西，或者三者来源于同一个X，三者同源。通过X找到X里面的关键点，从而更关注X的关键信息，忽略X的不重要信息。不是输入语句和输出语句之间的注意力机制，而是输入语句内部元素之间或者输出语句内部元素之间发生的注意力机制。

注意力机制和自注意力机制的区别：

 （1）注意力机制的Q和K是不同来源的，例如，在Encoder-Decoder模型中，K是Encoder中的元素，而Q是Decoder中的元素。在中译英模型中，中文句子通过编码器被转化为一组特征表示K，这些特征表示包含了输入中文句子的语义信息。解码器在生成英文句子时，会使用这些特征表示K以及当前生成的英文单词特征Q来决定下一个英文单词是什么。

（2）自注意力机制的Q和K则都是来自于同一组的元素，例如，在Encoder-Decoder模型中，Q和K都是Encoder中的元素，即Q和K都是中文特征，相互之间做注意力汇聚。也可以理解为同一句话中的词元或者同一张图像中不同的patch，这都是一组元素内部相互做注意力机制，因此，自注意力机制（self-attention）也被称为内部注意力机制（intra-attention）。

2.2 如何运用自注意力机制？ 
其实步骤和注意力机制是一样的。

第1步：得到Q，K，V的值

对于每一个向量x，分别乘上三个系数 ， ，，得到的Q，K和V分别表示query，key和value



【注意】三个W就是我们需要学习的参数。

 第2步：Matmul

利用得到的Q和K计算每两个输入向量之间的相关性，一般采用点积计算，为每个向量计算一个score：score =q · k 



第3步：Scale+Softmax

将刚得到的相似度除以，再进行Softmax。经过Softmax的归一化后，每个值是一个大于0且小于1的权重系数，且总和为1，这个结果可以被理解成一个权重矩阵。



第4步：Matmul

使用刚得到的权重矩阵，与V相乘，计算加权求和。



以上是对Thinking Machines这句话进行自注意力的全过程，最终得到z1和z2两个新向量。

其中z1表示的是thinking这个词向量的新的向量表示（通过thinking这个词向量，去查询和thinking machine这句话里面每个单词和thinking之间的相似度）。

也就是说新的z1依然是 thinking 的词向量表示，只不过这个词向量的表示蕴含了 thinking machines 这句话对于 thinking 而言哪个更重要的信息。

2.3 自注意力机制的问题
自注意力机制的原理是筛选重要信息，过滤不重要信息，这就导致其有效信息的抓取能力会比CNN小一些。这是因为自注意力机制相比CNN，无法利用图像本身具有的尺度，平移不变性，以及图像的特征局部性（图片上相邻的区域有相似的特征，即同一物体的信息往往都集中在局部）这些先验知识，只能通过大量数据进行学习。这就导致自注意力机制只有在大数据的基础上才能有效地建立准确的全局关系，而在小数据的情况下，其效果不如CNN。

另外，自注意力机制虽然考虑了所有的输入向量，但没有考虑到向量的位置信息。在实际的文字处理问题中，可能在不同位置词语具有不同的性质，比如动词往往较低频率出现在句首。

要唠这个这就唠到位置编码(Positional Encoding) 了，这个我们下篇论文里面再讲，先大致说一下吧：对每一个输入向量加上一个位置向量e，位置向量的生成方式有多种，通过e来表示位置信息带入self-attention层进行计算。

具体原理吧，感兴趣的话可以看一下：

[2003.09229] Learning to Encode Position for Transformer with Continuous Dynamical Model (arxiv.org)

🌟三、多头注意力机制：Multi-Head Self-Attention
通过刚才的学习，我们了解到自注意力机制的缺陷就是，模型在对当前位置的信息进行编码时，会过度的将注意力集中于自身的位置，有效信息抓取能力就差一些。 因此就有大佬提出了通过多头注意力机制来解决这一问题。这个也是实际中用的比较多的。

3.1 什么是多头注意力机制？
在实践中，当给定相同的查询、键和值的集合时， 我们希望模型可以基于相同的注意力机制学习到不同的行为， 然后将不同的行为作为知识组合起来， 捕获序列内各种范围的依赖关系 （例如，短距离依赖和长距离依赖关系）。 因此，允许注意力机制组合使用查询、键和值的不同 子空间表示（representation subspaces）可能是有益的。

为此，与其只使用单独一个注意力汇聚， 我们可以用独立学习得到的h组（一般h=8）不同的线性投影（linear projections）来变换查询、键和值。 然后，这h组变换后的查询、键和值将并行地送到注意力汇聚中。 最后，将这h个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。 这种设计被称为多头注意力（multihead attention）。



3.2 如何运用多头注意力机制？ 
第1步：定义多组W，生成多组Q、K、V

刚才我们已经理解了，Q、K、V是输入向量X分别乘上三个系数 ， ，分别相乘得到的，  ， ，是可训练的参数矩阵。

现在，对于同样的输入X，我们定义多组不同的 ， ， ，比如、、，、、每组分别计算生成不同的Q、K、V，最后学习到不同的参数。



第2步：定义8组参数

对应8个single head，对应8组  ， ， ，再分别进行self-attention，就得到了-



第3步：将多组输出拼接后乘以矩阵以降低维度

首先在输出到下一层前，我们需要将-concat到一起，乘以矩阵做一次线性变换降维，得到Z。



 完整流程图如下：（感谢翻译的大佬！）



【注意】对于上图中的第2）步，当前为第一层时，直接对输入词进行编码，生成词向量X；当前为后续层时，直接使用上一层输出。 

🌟四、通道注意力机制：Channel Attention
（恭喜你已经翻越了3座大山看到这里 (๑•̀ㅂ•́)و✧）

4.1 什么是通道注意力机制？
对于输入2维图像的CNN来说，一个维度是图像的尺度空间，即长宽，另一个维度就是通道，因此通道注意力机制也是很常用的机制。通道注意力旨在显示的建模出不同通道之间的相关性，通过网络学习的方式来自动获取到每个特征通道的重要程度，最后再为每个通道赋予不同的权重系数，从而来强化重要的特征抑制非重要的特征。

使用通道注意力机制的目的：为了让输入的图像更有意义，大概理解就是，通过网络计算出输入图像各个通道的重要性（权重），也就是哪些通道包含关键信息就多加关注，少关注没什么重要信息的通道，从而达到提高特征表示能力的目的。



 咦，我们好像看到了一个老朋友——SENet！ ╰(*°▽°*)╯

4.2 SENet
SE注意力机制（Squeeze-and-Excitation Networks）在通道维度增加注意力机制，关键操作是squeeze和excitation。

通过自动学习的方式，即使用另外一个新的神经网络，获取到特征图的每个通道的重要程度，然后用这个重要程度去给每个特征赋予一个权重值，从而让神经网络重点关注某些特征通道。提升对当前任务有用的特征图的通道，并抑制对当前任务用处不大的特征通道。

如下图所示，在输入SE注意力机制之前（左侧白图C2），特征图的每个通道的重要程度都是一样的，通过SENet之后（右侧彩图C2），不同颜色代表不同的权重，使每个特征通道的重要性变得不一样了，使神经网络重点关注某些权重值大的通道。


这里就简单回顾一下~

更详细的了解请看我的这两篇：

经典神经网络论文超详细解读（七）——SENet（注意力机制）学习笔记（翻译＋精读＋代码复现） SENet代码复现＋超详细注释（PyTorch）

4.3 其他通道注意力机制
① ECA
YOLOv5改进系列（4）——添加ECA注意力机制_路人贾'ω'的博客-CSDN博客

ECA 注意力机制，它是一种通道注意力机制；常常被应用与视觉模型中。支持即插即用，即：它能对输入特征图进行通道特征加强，而且最终ECA模块输出，不改变输入特征图的大小。

背景：ECA-Net认为：SENet中采用的降维操作会对通道注意力的预测产生负面影响；同时获取所有通道的依赖关系是低效的，而且不必要的；
设计：ECA在SE模块的基础上，把SE中使用全连接层FC学习通道注意信息，改为1*1卷积学习通道注意信息；
作用：使用1*1卷积捕获不同通道之间的信息，避免在学习通道注意力信息时，通道维度减缩；降低参数量；（FC具有较大参数量；1*1卷积只有较小的参数量）


② CBAM
YOLOv5改进系列（2）——添加CBAM注意力机制_路人贾'ω'的博客-CSDN博客

CBAM全称Convolutional Block Attention Module，这是一种用于前馈卷积神经网络的简单而有效的注意模块。是传统的通道注意力机制+空间注意力机制，是 channel(通道) + spatial(空间) 的统一。即对两个Attention进行串联，channel 在前，spatial在后。

给定一个中间特征图，我们的模块会沿着两个独立的维度（通道和空间）依次推断注意力图，然后将注意力图乘以输入特征图以进行自适应特征修饰。 由于CBAM是轻量级的通用模块，因此可以以可忽略的开销将其无缝集成到任何CNN架构中，并且可以与基础CNN一起进行端到端训练。



🌟五、空间注意力机制：Spatial Attention
5.1 什么是空间注意力机制？
其实上面那个图就包含空间注意力机制了：绿色长条的是通道注意力机制，而紫色平面则就是空间注意力机制。

不是图像中所有的区域对任务的贡献都是同样重要的，只有任务相关的区域才是需要关心的，比如分类任务的主体，空间注意力模型就是寻找网络中最重要的部位进行处理。空间注意力旨在提升关键区域的特征表达，本质上是将原始图片中的空间信息通过空间转换模块，变换到另一个空间中并保留关键信息，为每个位置生成权重掩膜（mask）并加权输出，从而增强感兴趣的特定目标区域同时弱化不相关的背景区域。

5.2 STN
STN《Spatial Transformer Networks》是15年NIPS上的文章STN引入了一个新的可学习的空间转换模块，提出了空间变换器（Spatial Transformer）的概念，它可以使模型具有空间不变性。这个可微分模块可以插入到现有的卷积结构中，使神经网络能够在Feature Map本身的条件下自动地对特征进行空间变换，而无需任何额外的训练监督或优化过程的修改。主要作用是找到图片中需要被关注的区域，并对其旋转、缩放，提取出固定大小的区域。



空间采样器的实现主要分成三个部分:

1）局部网络（Localisation Network）
2）参数化网格采样( Parameterised Sampling Grid)
3）差分图像采样（Differentiable Image Sampling）
总结
以上就是我们这篇要介绍的注意力机制，目前所有的注意力机制方法大都是基于各个不同的维度利用有限的资源进行信息的充分利用，它本质作用是增强重要特征，抑制非重要特征。注意力机制非常重要，在CV领域可以说是遍地开花，被广泛应用在网络中提升模型精度，本文也只是简单的介绍了一下，为下一篇论文阅读扫清障碍。以后应用于代码之中再详细介绍吧！
一、grep命令的基本概念和用途
grep命令是linux中一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。
在一个或多个文件中搜素字符串模式，如果字符串模式包括空格，也必须被引用，模式后的所有字符串被看作文件名。搜索的结果被送到标准输出（stdout)，不影响原文件内容。

二、grep命令的命令格式
grep [option] pattern files
AI写代码
1
根据上面的命令格式，我们可以了解到grep命令主要有两个部分[option]和pattern，下面分别从这两个部分开始去了解熟悉grep命令。

1、主要选项[option]说明及示例
参数选项	解释说明
-c	只统计匹配的行数
-v	排除匹配结果
-n	显示匹配行与行号
-i	不区分大小写
-E	使用egrep命令
-color = auto	为grep过滤结果添加颜色
-w	只匹配过滤的单词
-o	只输出匹配的内容
示例：
测试文件：test.c


过滤test.c文件中hello内容
grep hello test.c

统计test.c中hello内容的行数
grep -c hello test.txt

过滤test.c文件中hello内容并显示行号
grep -n hello test.c
排除test.c文件中匹配结果为hello的内容
grep -v hello test.c

过滤test.c文件中hello内容,不区分大小写
grep -i hello test.c

只输出匹配结果为hello的内容
grep -o hello test.c


2、grep、cat、ps、管道符配合使用
过滤test.c文件中hello内容
cat test.c | grep hello


过滤进程信息
ps -ef | grep renhui

ps 打印进程信息
renhui 过滤关键字
|管道符，将一个命令的输出作为另外一个命令的输入
AI写代码
1
2
3


3、[pattern]正则表达式主要参数[即元字符]说明和示例
-? : 同时显示匹配行的上下？行
grep -n -2 hello test.c


^ : 匹配正则表达式的以“某字符串”开头的行
grep "^int" test.c

$ : 匹配正则表达式的以“某字符串”结尾的行
grep ");$" test.c

< : 从匹配正则表达式的行开始
grep "\<h" test.c

> : 到匹配正则表达式的行结束
> grep "n\>" test.c

[ ] : 单个字符，如[A] 即A 符合要求
grep [a] test.c

[^ ] : 显示不包括括号中字符串的所有行
grep [^n] test.c

[ - ] : 范围匹配，如[A-C]，即A、B、C都符合要求；如[a,b]，即只有a和b符合要求
grep [a-d] test.c

另外有些字符类描述，大家有兴趣可以去记一下
[:alnum:] 字母数字集 “a-z A-Z 0-9”
[:alpha:] 字母集合 “a-z A-Z”
[:blank:] 空格或制表键
[:cntrl:] 任何控制字符
[:digit:] 数字集合 “0-9”
[:graph:] 任何可视字符（无空格）
[:lower:] 小写字母 “a-z”
[:print:] 非控制字符
[:punct:] 标点字符
[:space:] 空格
[:upper:] 大写字母 “A-Z”
[:xdigit:] 十六进制数字 “0-9 a-f A-F”

4、在指定目录所有文件搜索关键字，并显示文件名
grep -r "hehe" .





编译原理学习笔记-3：词法分析（一）基本过程、正规式和有限自动机
原创
于 2020-04-11 23:29:40 发布
·
3.5k 阅读
·

6
·
 34
·
CC 4.0 BY-SA版权

编译原理
专栏收录该内容
3 篇文章
订阅专栏
本文深入解析词法分析的五大步骤，重点阐述词法分析器如何从源程序中生成单词流，包括关键字、标识符、常数、运算符和界符的识别。探讨了词法分析的要点，如预处理、超前扫描，以及状态转换图和有限自动机模型在词法分析中的应用。
这是关于编译原理的第三篇笔记。



编译有五大步骤，本篇笔记将会讲解编译的第一步：词法分析。

词法分析的任务是：从左往右逐个字符地扫描源程序，产生一个个的单词符号。也就是说，它会对输入的字符流进行处理，再输出单词流。执行词法分析的程序即词法分析器，或者说扫描器。

1.词法分析的成果
词法分析的成果就是由一系列单词符号构成的单词流。单词符号其实就是 token，一般有以下五大类：

关键字：例如 while，if，int 等
标识符：变量名、常量名、函数名等
常数：例如，100，'text'，TRUE 等
运算符：例如 +，*，/ 等
界符：逗号，分号，括号，点等
具体来说，一个单词符号在形式上是这样的一个二元式：（单词种别，单词符号的属性值）。

单词种别：

单词种别通常用整数编码。一个语言的单词符号如何分种，分成几种，怎样编码是一个技术问题。它取决于处理上的方便。

标识符一般统归为一种。比如说变量 a 和 b，可能我们都只用 1 作为它们的单词种别。

常数则宜按类型（整、实、布尔等）分种，比如说整数可能用 2 表示，布尔值可能用 3 表示。

关键字可以把全体视为一种，也可以一字一种。

运算符可以把具有一定共性的运算符视为一种，也可以一符一种。

界符一般是一符一种。

单词符号的属性值

由上面的单词种别可以知道，关键字、运算符、界符基本都是一字（或者一符）对应一个种别，所以只依靠单词种别即可确切地判断出具体是哪一种单词符号了。但是标识符和常数却不是这样，一个种别可能对应好几个单词符号。所以我们需要借助单词符号的属性值做进一步的区分。

对于标识符类型的单词符号，它的属性值通常是一个指针，这个指针指向符号表的某个表项，这个表项包含了该单词符号的相关信息；对于常数类型的单词符号，它的属性值也是一个指针，这个指针指向常数表的某个表项，这个表项包含了该单词符号的相关信息。

以 while(i>=j)i++ 为例，它的单词符号流大概如下：

<while,->
<(,->
<id,pointer1>  
<>=,->  
<id,pointer2>  
<),->  
<id,pointer3>
<++,->    
<;,->    
AI写代码
js
1
2
3
4
5
6
7
8
9
注意：实际上，对于关键字、界符这些，应该用整数表示单词种别，不过这里为了便于区分，直接用对应的单词符号表示了。对于标识符，由于 id 这个单词种别可能对应多个标识符，所以可以看到我们用不同的指针进行了标识。其它不需要标识的，则统一用短横线代替。

2. 词法分析的要点
2.1 是否作为一趟？
按照我们常规的想法，应该是词法分析器扫描整个源程序，产生单词流，之后再由语法分析器分析生成的单词。如果是这样，那么就说词法分析器独立负责了一趟的扫描。但其实，更多的时候我们认为词法分析器并不负责独立的一趟，而是作为语法分析器的子程序被调用。也就是说，一上来就准备对源程序进行语法分析，但是语法分析无法处理字符流，所以它又回过头调用了词法分析器，将字符流转化成单词流，再去分析它的语法。以此类推，后面每次遇到字符串流，都是这样的一个过程。

2.2 输入和预处理
字符流输入后首先到达输入缓冲区，在词法分析器正式对它进行扫描之前，还得先做一些预处理的工作。预处理子程序会对一定长度的字符流进行处理，包括去除注释、合并多个空白符、处理回车符和换行符等。处理完之后再把这部分字符流送到扫描缓冲区。此时，词法分析器才正式开始拆分字符流的工作。

词法分析器对扫描缓冲区进行扫描时一般使用两个指示器：起点指示器指向当前正在识别单词的开始位置，搜索指示器用于向前搜索以寻找单词的终点。问题在于，就算缓冲区再大，也难保不会出现突破缓冲区长度的单词符号。也就是说，输入缓冲区把处理好的一段字符流送到扫描缓冲区时，扫描缓冲区可能装不下这段字符流，在这种情况下，如果依然只用一个缓冲区存放字符流，可能会导致某个过长的单词符号无法被正确读取。因此，扫描缓冲区最好使用如下一分为二的区域：



这样，在搜索指示器向前搜索到 A 半区边缘时，如果发现还没有找到单词符号的终点，那么就会调用预处理程序把剩下的部分送到 B 半区，搜索指示器再来到 B 半区扫描。这样就可以避免截断，从而将这个过长的单词符号顺利衔接起来。如果单词符号实在太长，两个半区都无法解决，那就没辙了。所以应该对单词符号的长度加以限制。

2.3 超前扫描
像 FORTRAN 这样的语言，关键字不加保护（只要不引起矛盾，用户可以用它们作为普通标识符），关键字和用户自定义的标识符或标号之间没有特殊的界符作间隔。这使得关键字的识别变得很麻烦。比如 DO99K=1,10 和 DO99K=1.10。前者的意思是，K 从 1 变到 10 之后，跳转到第 99 行执行；后者的意思是，为变量 DO99K 赋值 1.10。问题在于，我们并不能在扫描到 DO 的时候就肯定这是一个关键字，事实上，它既有可能是关键字，也有可能作为标识符的一部分。而具体是哪一种，只有在我们扫描到 =1 后面才能确定 —— 如果后面是逗号，则这是关键字，如果是点号，则是标识符的一部分。

也就是说，我们需要超前扫描到达第一个界符 =，但是 = 还不能确定，再继续超前扫描到达第二个界符（逗号或者点号），这时候才能完全确定。

3. 词法分析的模型
3.1 状态转换图
状态转换图是设计词法分析程序的一种模型，我们可以借助这个模型体会识别某个特定字符串的过程。它是一张有限方向图，结点表示状态，结点之间的箭弧上有字符，表示遇到该字符就将其读进来，并且转换到另一个状态。以下面这张图为例，在状态 0 下如果输入的是字母，则将字母读进来，并进入状态 1 ；在状态 1 下如果输入的是字母或者数字，则将其读进来并重新进入状态 1 。不断重复，直到输入的不是字母和数字，这时候也将其读进来，并进入状态 2。状态 2 是终态，有一个 * 作为标记，标记着多读进来一个不属于目标的符号，应该把它退还给原输入串。这张图实际表示的是标识符类型的输入串。



状态转换图的结点（状态）个数是有限的，其中有一个初态，以及至少一个终态（同心圆表示）。

左图是 FORTRAN 语言的一些单词符号，右图是对应的状态转换图：



状态转换图的实现：



比如上面的状态转换图，它的词法分析器大概如下：



3.2 正规式与有限自动机
状态转换图是制造词法分析器的模型，不过这个模型过于具体，我们应该想个办法，用一种更接近数学的、更为形式化的方法来表示状态转换图。而这种状态转化图的形式化表达，就是有限自动机。由于有限自动机涉及到了正规式、正规集等其它概念，所以我们这里先普及一下这些概念。

① 正规式与正规集
推导

正规式和正规集都是相对于字母表来说的概念，通常说“xx 字母表的正规式是…，字母表的正规集是…”。对于正规式和正规集，我们采用递归的方式进行定义。即，对于某个给定的字母表 ∑，规定：

ε 和 Ø 都是该字母表的正规式，这两个正规式分别表示了 {ε} 和 Ø 这两个正规集
字母表上的任意一个元素 a 都是字母表的正规式，它表示了 {a} 这个正规集
如果 a 和 b 都是字母表上的正规式，且分别表示了 L(a) 和 L(b) 这两个正规集。那么，(a|b)，(ab)，(a)* 也都是正规式，它们分别代表了 L(a)∪L(b)，L(a)L(b) 和 (L(a))* 这几个正规集。（笛卡尔积和闭包）
仅由有限次使用上面三条规则而得到的表达式才是字母表上的正规式，仅由这些正规式表示的字集才是字母表上的正规集
根据上面这四条规则，我们可以递归列举出某个字母表的正规式和对应的正规集

例如对于给定的字母表 ∑ = {a,b}，我们可以像下面这样推导出它的正规式和对应的正规集：

ba* ：a 是正规式，所以 a* 也是正规式（规则二），所以 ba* 也是正规式（规则二）。a 表示 {a} 这个集合，加上星号则表示该集合的闭包，b 表示 {b} 这个集合，所以并排放在一起表示两个集合作笛卡尔积运算

等价的正规式

如果两个正规式 U 和 V 表示的正规集相同，则认为这两个正规式等价，记作 U = V。例如，b(ab)* 和 (ba)*b 就是等价的两个正规式。它们表示的集合形如 {ba,bb,bab,babab,babababab,......}。可以看出这个集合的元素特点是，以 b 开头，后面跟着 a 和 b 自由组合的符号串。在没有引入正规式的概念之前，要表示这样的集合是比较麻烦的，但现在则方便很多。

正规式运算规则

对于正规式 U，V，W，它们满足下面的运算规律：

1、交换律：U|V＝V|U

2、结合律：U|(V|W)＝(U|V)|W

3、结合律：(UV)W=U(VW)

4、分配律：U(V|W) = UV|UW

5、分配律： (V|W)U = VU|WU

6、零一律：εU=U

7、零一律：Uε=U

最后再来看一道题：

令 ∑＝｛d，. ，e，＋，－｝，其中d为 0～9 中的数字，则 ∑ 上的正规式

d*(.dd*|ε)(e(+|-|ε)dd*|ε)表示的是？

先来划分结构，以 d* 开头，说明第一个部分是一个整数，第二个部分是 (.dd*|ε)，可以取空，第三个部分是 (e(+|-|ε)dd*|ε)，同样可以取空。如果后面两个部分都取空，则肯定代表一个整数；如果第二个部分不取空，则会出现小数点，表明这时候会是一个小数；如果第三个部分不取空，则会出现 e，表明这是一个用科学计数法表示的数字。综上，这个正规式表示的是所有无符号数构成的集合。

有个需要注意的地方是，d* 已经可以表示所有整数了，为什么小数点后使用的是 dd* 而不是 d* 呢？这里其实是起到一个占位的作用，因为单纯用 d* 的话，其实也包括了空符号串，但是既然出现了小数点，后面至少要跟一位数，不能为空。所以这里用 dd*。对于 e 后面也是同理，既然出现了 e，后面就不能为空了。

② 确定有限自动机
1. 确定有限自动机的结构

我们先来回顾一下这副状态转换图：



考虑到要用形式化的方法来表示它，我们得先考虑转换图的一些重要组成因素。

首先想到的是，必须得有一个集合用来保存所有的状态
还需要有一个集合用来保存所有的输入字符
在某一个状态下，根据输入的字符不同，会跳转到不同的状态，这三者构成一个联系，多个联系自然也需要保存起来
初态是特殊的，需要单独保存
终态也是特殊的，需要单独保存
那么，我们可以构造一个有限的状态集合 S ，用以保存该转换图的所有状态；构造一个有限的字母集合 ∑，用以保存每一个输入的字符；构造包括多个单值映射对 的 δ，每一对都表示从“当前状态和输入字符”到“跳转状态”的映射关系。具体地说，用 δ(s,a) = a' 表示，当前状态为 s 且输入字符为 a 时，跳转到状态 a'；此外，需要用来自于状态集合 S 的 s0 作为唯一的初态；最后，构造一个终态集合 F，它是 S 的子集，可取空。

这样，我们就有了 S，∑，δ，s0，F。这五个元素在一起就构成了我们要讲的是确定有限自动机。即，确定有限自动机 DFA 可用如下的五元式表示：

M = {S，∑，δ，s0，F}

2. 确定有限自动机的其它表示

正如我们所说的，有限自动机是抽象层面上的形式化表达，而它在具体层面上的表达就是之前所讲的状态转换图。另外，确定有限自动机还可以用一个矩阵来表示，这样的矩阵即 状态转换矩阵。它的行表示当前状态，列表示输入字符，而矩阵元素则表示跳转状态，也就是 δ(s,a) 的值。

以 DFA M = （{0，1，2，3，4}，{a,b}，δ，0，{3}） 为例，如果它的映射如下：

δ(0,a) = 1  δ(0,b) = 2

δ(1,a) = 3  δ(1,b) = 2

δ(2,a) = 1  δ(2,b) = 3

δ(3,a) = 3  δ(3,b) = 3
AI写代码
js
1
2
3
4
5
6
7
那么它的状态转换矩阵如下所示：

当前状态	a	b
0	1	2
1	3	2
2	1	3
3	3	3
3. 确定有限自动机的作用

确定有限自动机是状态转换图的形式化表达，它可以用于识别（或者说读出、接受）正规集。

对于 ∑* 中的任何一个字 a，若存在一条从初态结点到某一终态结点的通路，且这条通路上所有箭弧的标记符连接成的字等于 a，则称 a 为 DFA M 所识别（读出或接受）。

如果 M 的初态结点同时也是终态结点，那么就说空符号串可以被 M 所识别。

DFA M 可以识别的字的全体记为 L(M)。

看下面的例子：



这是某个确定有限自动机对应的状态转换图，那么这个 DFA M 可以识别什么样的正规集呢？我们可以先走几条路线看看（假定在遇到状态 3 就停止），不难发现它可以识别出诸如 aa，bb，abb，baa 这样的符号串。这样的符号串的特点是，中间要么是 aa ，要么是 bb，所以首先确定中间是 (aa|bb)。由于 aa 和 bb 都可以独立存在，说明 (aa|bb)的前面和后面必须可以是空符号串，说到空符号串，我们会想到闭包，所以它的前面后面必定会分别出现一个闭包。考虑前面，可以出现 a 或者 b，所以前面应该是 (a|b)*；考虑后面，我们在遇到状态 3 的时候就停止了，但实际上，在这之后遇到 a 或者 b，状态变化会循环往复，也就是说，不管遇到什么样的 ab 组合符号串，都能够被识别并循环转换到状态 3，这里说明后面的状态是任意的，所以确定后面是 (a|b)*。

结合起来，这个有限自动机可以识别的正规集可以用正规式 (a|b)*(aa|bb)(a|b)* 表示。

③ 非确定有限自动机
1. “确定”和“不确定”指的是什么？

“确定”指的是，五元式中的映射是一个单值函数，也就是说，在当前状态下，面对某个输入字符，其跳转状态是唯一确定的，即只会跳转到某一个值。但是，有的时候映射是多值函数，也就是说，在某个输入字符下有多个跳转状态可供选择。具有这样特点的有限自动机，就叫做非确定有限自动机。

2. 非确定有限自动机的结构

非确定有限自动机可以用如下的五元式表示：

M = {S，∑，δ，s0，F}

S 仍然是状态集合，∑ 仍然是输入字符集合，F 仍然是终态集合。
但是，s0 不再表示单个初态，而是表示一个非空的初态集合
另外，正如前面所说的，δ 不再是一个从“当前状态和输入字符”到“跳转状态”的单值映射，而是从**“当前状态和输入字符集合闭包”到“跳转状态集合”**的子集映射。简单地说就是，它接受的不一定是单个字符，且在单一输入下可以跳转到多个状态
3. 非确定有限自动机的作用

非确定有限自动机同样可以用于识别（或者说读出、接受）正规集。

对于 ∑* 中的任何一个字 a，若存在一条从初态结点到某一终态结点的通路，且这条通路上所有箭弧的标记符连接成的字等于 a，则称 a 为 NFA M 所识别（读出或接受）。

如果 M 的初态结点同时也是终态结点，或者存在一条从某个初态结点到某个终态结点的 ε 通路，那么就说空符号串 ε 可以被 M 所识别。（因为输入符号来自于集合闭包，所以输入符号接受空符号串 ε）

看下面的例子：

假设有非确定有限自动机 NFA M＝({0,1,2,3,4},{a,b},δ,{0},{2,4})，其中，

δ(0,a)={0,3}    δ(2,b)={2}

δ(0,b)={0,1}    δ(3,a)={4}

δ(1,b)={2}      δ(4,a)={4}

δ(2,a)={2}      δ(4,b)={4}
AI写代码
js
1
2
3
4
5
6
7
可以看到，有不少 δ 是被映射到 S 的一个子集，而不是像确定 DFA 那样映射到一个输入字符。这个 NFA 对应的状态转换图如下：



这里会发现，这个 NFA 所能识别的正规集和之前的 DFA 是一样的，都是含有相继两个 a 或者相继两个 b 的符号串。事实上，尽管 DFA 是 NFA 的特例，但是对于每个 NFA M，都会有一个 DFA M‘ 与之对应，使得 L(M) = L(M')。这时候，我们就说 NFA M 等价于 DFA M’。

③ 非确定有限自动机的确定化
非确定有限自动机的确定化，指的就是将非确定有限自动机转换为一个与之等价的确定有限自动机。总的来说分为两步，第一步是利用等价转换规则细化 NFA 状态转换的过程；第二步是利用子集法对第一步转化得到的 NFA 进行确定化。由于第二步又涉及到了一些概念，所以这里我们先来对这些概念进行解释。

相关概念：
（1）空闭包集合

若 I 是一个状态集合的子集，那么 I 会有一个空闭包集合，记作 ε-closure(I)。这个空闭包集合同样是一个状态集合，它的元素符合以下几点：

I 的所有元素都是空闭包集合的元素
对于 I 中的每一个元素，从该元素出发经过任意条 ε 弧能够到达的状态，都是空闭包集合的元素
以下面这张图为例：



ε-closure({5,3,4}) 会等于多少呢？这里的 I 是 {5,3,4}，所以空闭包集合一定包含了5，3，4。从 5 出发，经过一条 ε 弧到达 6，两条 ε 弧到达 2，所以 6 和 2 也是闭包集合的元素；从 3 出发，经过一条 ε 弧到达 8，所以 8 也是；从 4 出发，经过一条 ε 弧 7，所以 7 也是。综上，ε-closure({5,3,4}) = {5，3，4，6，2，8，7} 。

（2）Ia

若 I 是一个状态集合的子集，那么它相对于状态 a 的 Ia 等于 ε-closure(J)。其中，J 表示的是，从 I 中每个状态出发，经过标记为 a 的单条弧而到达的状态的集合。也就是说，Ia 表示的是从 I 中每个状态出发，经过标记为 a 的弧而到达的状态，再加上从这些状态出发，经过任意条 ε 弧能够到达的状态。

还是以这幅图为例：



当 I 是 {1，2} 的时候，Ia 等于多少呢？

从 1 出发，经过 a 弧能够到达 5 和 4，所以 5，4 属于 Ia。从 5，4 出发，经过 ε 弧能够到达 6，2，7，所以 6，2，7 属于 Ia
从 2 出发，经过 a 弧能够到达 3，所以 3 属于 Ia。从 3 出发，经过 ε 弧能够到达 8，2，7，所以 8 属于 Ia
综上，Ia = {5,4,6,2,7,3,8}

下面，介绍具体的确定化过程。

第一步：规则转换


第一条和第二条都好理解，重点在第三条规则。为什么右边的图可以等价于左边的图呢？A* 其实表示的是类似 {ε，A，AA，AAA，AAAA，......} 这样的集合，因为 A 自由组合形成的符号串是可以用一个 A 的自循环来表示的，所以中间有一个自循环，而 ε 则可以用 εε 来表示，所以考虑在前后各加一个 ε，对于 A 的符号串不影响。

第二步：子集法转换
子集法的核心是，针对上面规则转换后得到的 NFA，画出它的状态转换矩阵，这个矩阵的矩阵元素是映射的子集，不是单值，而我们要做的事情就是把这个子集用一个单值来表示。也就是说，对于 NFA 的每一组映射状态集，都用一个来自 DFA 的映射单值与之对应，从而求出等价的 DFA。

假设经过第一步，我们已经得到下面的 NFA：



选取 NFA 的初态集合的空闭包作为初始集合 I，这个集合 I 将是 ε-closure({i}) = {i,1,2} 。同时由于输入符号只有 a 和 b，所以第二列为 Ia ，第三列为 Ib。得到如下这个表：

Ia	Ib
{i，1，2}		
根据前面的说法求解 Ia 和 Ib。从 i 出发没有 a 弧，无视之；从 1 出发经过 a 弧 到达 1，从 2 出发经过 a 弧到达 3；从 1 出发经过 ε 弧到达 2，从 1 出发没有 ε 弧。所以，Ia = {1,2,3}。从 i 出发没有 b 弧，无视之；从 1 出发经过 b 弧到达 1，从 2 出发经过 b 弧到达 4。从 1 出发经过 ε 弧到达 2，从 4 出发没有 ε 弧，所以 Ib = {1,2,4}。记新得到的两个

集合为 A 和 B，得到下面的表：

Ia	Ib
{i，1，2}	A：{1,2,3}	B：{1,2,4}
将新得到的集合 A 和 B 作为第一列的元素，得到下面的表：

Ia	Ib
{i，1，2}	A：{1,2,3}	B：{1,2,4}
A：{1,2,3}		
B：{1,2,4}		
分别对 A 集合和 B 集合求解对应的 Ia 和 Ib，得到下表（对于同样形式的集合仍采取之前命名，仅对新出现集合给定新的命名）：

Ia	Ib
{i，1，2}	A：{1,2,3}	B：{1,2,4}
A：{1,2,3}	C：{1,2,3,5,6,f}	B：{1,2,4}
B：{1,2,4}	A：{1,2,3}	D：{1,2,4,5,6,f}
将新得到的 C、D 集合作为第一列的元素，同样求解 Ia 和 Ib，得到下面的表：

Ia	Ib
{i，1，2}	A：{1,2,3}	B：{1,2,4}
A：{1,2,3}	C：{1,2,3,5,6,f}	B：{1,2,4}
B：{1,2,4}	A：{1,2,3}	D：{1,2,4,5,6,f}
C：{1,2,3,5,6,f}	C：{1,2,3,5,6,f}	E：{1,2,4,6,f}
D：{1,2,4,5,6,f}	F：{1,2,3,6,f}	D：{1,2,4,5,6,f}
同理，继续推导，直到再也没有新集合出现：

Ia	Ib
{i，1，2}	A：{1,2,3}	B：{1,2,4}
A：{1,2,3}	C：{1,2,3,5,6,f}	B：{1,2,4}
B：{1,2,4}	A：{1,2,3}	D：{1,2,4,5,6,f}
C：{1,2,3,5,6,f}	C：{1,2,3,5,6,f}	E：{1,2,4,6,f}
D：{1,2,4,5,6,f}	F：{1,2,3,6,f}	D：{1,2,4,5,6,f}
E：{1,2,4,6,f}	F：{1,2,3,6,f}	D：{1,2,4,5,6,f}
F：{1,2,3,6,f}	C：{1,2,3,5,6,f}	E：{1,2,4,6,f}
现在，用字母命名代替所有的集合（初始集合给定名字 S），得到下面的矩阵：

Ia	Ib
S	A	B
A	C	B
B	A	D
C	C	E
D	F	D
E	F	D
F	C	E
这个矩阵实际上已经是一个 DFA 矩阵。我们再以初始集合 S 将作为初态，包含原始 NFA 终态的集合（即 C、D、E、F）作为终态，画出它对应的状态转换图，如下：



那么，这个转换图实际上就是与最初 NFA 等价的 DFA 所对应的转换图了，到这里，我们就完成了对非确定有限自动机进行确定化的工作了。

最后我们再对这篇笔记涉及的知识点做一下回顾。首先我们解释了词法分析的结果，也就是单词符号，之后讲解了一些词法分析过程中的要点（预处理、超前扫描），最后则是本篇笔记的重点，词法分析的模型，包括状态转换图以及它的形式化表达 —— 有限自动机。







Xv6--页表
原创
于 2023-12-20 19:06:22 发布
·
1.9k 阅读
·

32
·
 47
·
CC 4.0 BY-SA版权
文章标签：
#linux
#c语言

本文详细阐述了Xv6操作系统中的页表结构，包括其功能、在RISC-VSv39架构下的硬件支持，以及Xv6中页表的使用方法和实现，重点讲解了一级、二级和三级页表的寻址过程。
一、页表 Page tables
1、功能
xv6对此的描述：

Page tables are the mechanism through which the operating system provides each process with its own private address space and memory. 
Page tables determine what memory addresses mean, and what parts of physical memory can be accessed. 
They allow xv6 to isolate different process’s address spaces and to multiplex them onto a single physical memory.
Page tables also provide a level of indirection that allows xv6 to perform a few tricks: 
mapping the same memory (a trampoline page) in several address spaces, and guarding kernel and user stacks with an unmapped page. 
AI写代码
Eng
1
2
3
4
5
总结一下:页表是操作系统提供的一种机制，通过页表，操作系统可以隔离每个进程的地址空间；可以让进程使用比物理内存更多的内存；可以实现再进程之间共享数据。
额外说明：地址空间
物理地址：物理地址就是真实的内存地址，就是CPU可以读写的内存位置，（这里又涉及到 独立编址和统一编址，此处不在详述）
虚拟地址：可以理解成，这个地址实际是不存在的，CPU需要MMU和页表将虚拟地址转换成物理地址才能正常工作，那为什么要设计虚拟地址呢？答案就是页表的功能。
内核地址空间：Xv6分为内核态和用户态，内核地址空间，就是在内核态使用的虚拟地址
用户地址空间：在用户态下使用的虚拟地址

2、硬件支持
Xv6运行在Sv39 RISC-V，64位机器，故CPU寻址是64位，但是只使用了低39位，高位保留。操作系统将页表地址填入satp寄存器，打开MMU，之后CPU会将加载的地址都认为是虚拟地址，寻址时，MMU回将该地址翻译成物理地址，再返回对应内存的值或将值写入正确的内存位置。

3、页表使用说明
在Xv6中，如果是设置一级页表，则39位地址中，12-39位作为page index，页表PTE索引
现在以一级页表为例来说明如何寻址：
VA：virtual address
VA：0x1111001100 = (001 0001 0001 0001 0000 0000 0001 0001 0000 0000)b 39位地址1111001100
VA[12:39] :(001 0001 0001 0000 0000 0001)=0x111001
(page_table[VA[12:39]] << 12) | VA[11:00] = PA[39:00]
可以看到Page Table之所以叫页表，其实他就是真实的存在在内存中的一个表，在操作系统初始化期间，会手动填充这个表，在一级页表中，寻址长度位39位，其中高27位是PTE索引，所以在操作系统初始化期间，会申请一块内存存储这个表，这段内存的大小可以计算出：
表我们可以理解为一个数组，数组的每一项就是物理地址的高27位，这里就算作8字节，一共有0x7FFFFF项，故需要申请 80x7FFFFF个字节的空间，也就是82^27Bytes。可以看到一级页表对内存造成了很大的浪费，故前人们又想出了多级页表的概念。
这里以三级页表为例来说明
同样的将虚拟地址空间分段，在三级页表中，64位地址使用了低39位，高27位分为三段，每段9位，则可以推算出，每一级页表需要82^7Bytes = 8512Bytes，所以每一级页表有512项PTE，51283的内存大小，这大大减少了页表所占用的内存大小；低12位地址，为页表offset，这里用低12为作为offset的原因:页表：首先我们会将内存的地址空间，也就是可以寻址的内存范围，抽象成 ”页“， 每个 页 又设定成 4096个字节也即0x1000，那么我们知道计算机是从0开始计数，故 0 - 4096-1 一共4096项，即0xFFF刚好是十二位，所以通过低12位的offset，可以查找到一个页中的每一个地址。综上所述，通过三级页表，CPU拿到一个VA地址，将一级页表地址写入MMU中，MMU在一级页表中通过VA一级页表索引找出二级页表的地址，再通过VA中二级页表的索引找出三级页表，再通过VA中三级页表索引找出对应的页，再将VA低12位地址offset加上，就找到VA所对应的PA物理地址。
VA[38:30] 9位为第一级页表的index
VA[29:21] 9位为第二级页表的index
VA[20:12] 9位为第三级页表的index
VA[11:00] 12位为页中offset

4、Xv6页表的实现
页表并不是一次性创建成功的，而是在使用的过程中创建的。xv6在建立内核地址空间时，将内核地址空间做了一个布局，可以看到左边是我们要实现的虚拟地址空间，右边时物理地址空间，基本时一个一一映射的关系，区别是在虚拟地址空间将Trampoline和Kstack映射到最高地址，这里设计成这样涉及到中断的相关知识，以后有机会另起一文来补充。


5、代码
main() --> kvminit() --> kvmmake() --> kvmmap() --> mappages() --> walk()

// Make a direct-map page table for the kernel.
pagetable_t
kvmmake(void)
{
  pagetable_t kpgtbl;

  kpgtbl = (pagetable_t) kalloc();
  memset(kpgtbl, 0, PGSIZE);

  // uart registers
  kvmmap(kpgtbl, UART0, UART0, PGSIZE, PTE_R | PTE_W);

  // virtio mmio disk interface
  kvmmap(kpgtbl, VIRTIO0, VIRTIO0, PGSIZE, PTE_R | PTE_W);

  // PLIC
  kvmmap(kpgtbl, PLIC, PLIC, 0x400000, PTE_R | PTE_W);

  // map kernel text executable and read-only.
  kvmmap(kpgtbl, KERNBASE, KERNBASE, (uint64)etext-KERNBASE, PTE_R | PTE_X);

  // map kernel data and the physical RAM we'll make use of.
  kvmmap(kpgtbl, (uint64)etext, (uint64)etext, PHYSTOP-(uint64)etext, PTE_R | PTE_W);

  // map the trampoline for trap entry/exit to
  // the highest virtual address in the kernel.
  kvmmap(kpgtbl, TRAMPOLINE, (uint64)trampoline, PGSIZE, PTE_R | PTE_X);

  // allocate and map a kernel stack for each process.
  proc_mapstacks(kpgtbl);

  return kpgtbl;
}

// add a mapping to the kernel page table.
// only used when booting.
// does not flush TLB or enable paging.
void
kvmmap(pagetable_t kpgtbl, uint64 va, uint64 pa, uint64 sz, int perm)
{
  if(mappages(kpgtbl, va, sz, pa, perm) != 0)
    panic("kvmmap");
}

// Create PTEs for virtual addresses starting at va that refer to
// physical addresses starting at pa. va and size might not
// be page-aligned. Returns 0 on success, -1 if walk() couldn't
// allocate a needed page-table page.
int
mappages(pagetable_t pagetable, uint64 va, uint64 size, uint64 pa, int perm)
{
  uint64 a, last;
  pte_t *pte;

  if(size == 0)
    panic("mappages: size");

  a = PGROUNDDOWN(va);
  last = PGROUNDDOWN(va + size - 1);
  for(;;){
    if((pte = walk(pagetable, a, 1)) == 0)
      return -1;
    if(*pte & PTE_V)
      panic("mappages: remap");
    *pte = PA2PTE(pa) | perm | PTE_V;
    if(a == last)
      break;
    a += PGSIZE;
    pa += PGSIZE;
  }
  return 0;
}

// Return the address of the PTE in page table pagetable
// that corresponds to virtual address va.  If alloc!=0,
// create any required page-table pages.
//
// The risc-v Sv39 scheme has three levels of page-table
// pages. A page-table page contains 512 64-bit PTEs.
// A 64-bit virtual address is split into five fields:
//   39..63 -- must be zero.
//   30..38 -- 9 bits of level-2 index.
//   21..29 -- 9 bits of level-1 index.
//   12..20 -- 9 bits of level-0 index.
//    0..11 -- 12 bits of byte offset within the page.
pte_t *
walk(pagetable_t pagetable, uint64 va, int alloc)
{
  if(va >= MAXVA)
    panic("walk");

  for(int level = 2; level > 0; level--) {
    pte_t *pte = &pagetable[PX(level, va)];
    if(*pte & PTE_V) {
      pagetable = (pagetable_t)PTE2PA(*pte);
    } else {
      if(!alloc || (pagetable = (pde_t*)kalloc()) == 0)
        return 0;
      memset(pagetable, 0, PGSIZE);
      *pte = PA2PTE(pagetable) | PTE_V;
    }
  }
  return &pagetable[PX(0, va)];
}
AI写代码
C
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
实际上申请内存并填充页表的就是walk函数。我们的页表要做成的最终结果是：
这里我举例说明，UART0 地址为 0x10000000L，因为这里虚拟地址与物理地址一一对应，所以
VA = 0x10000000, VA[38:30] = 0 VA[29:21] = 0x80 VA[20:12] = 0 VA[11:00] = 0
PA = 0x10000000
现在申请页表地址，假设
一级页表地址为 0x87FFF000 二级页表地址0x87FFE000 三级页表0x87FFD000，每个页表一共512项，每一项共8个字节。
额外说明，页表项叫做PTE，我们知道页表地址一定是4096字节对齐的，所以PTE的低12位在做寻址时肯定会清零，所以PTE的低12位可以作为PTE属性，Xv6只使用了低10位来表示PTE的属性，比如 该内存可读、可写、可执行、是否共享、用户操作权限、内核操作权限等等属性，这里不详述。

现在通过代码执行可以得到
0x87FFF000[0] = ((0x87FFE000 >>12) << 10) | PTE_V.(PTE_V 0x1 << 0 表示该PTE有效）
0x87FFF000[0] = 21FF F801
可以看到，在一级页表的第一项中，存放了二级页表的地址，这里是存放在一级页表的第一项是因为VA[38:30]=0,虚拟地址的一级页表索引位0，同理，三级页表的地址存放在二级页表的0x80位置
0x87FFE000[128] = ((0x87FFD000 >> 12) << 10) | PTE_V (PTE有效）
0x87FFE000[128] = 0x21FFF401
同理物理地址所在的页存放在 三级页表的第0项，因为VA[20:12]=0
0x87FFD000[0] = ((0x10000000 >> 12) << 10) | PTE_V | PTE_W | PTE_R (PTE有效 可读 可写）
0x87FFD000[0] = 0x04000007
至此，我们就将UART0 0X10000000（VA） 映射到了0x10000000（PA） ，之后访问0x10000000就是访问实际的0x10000000这个地址。这就是我们需要用代码实现的过程，可以用gdb调试，最终结果于此相同，Xv6的实现是很巧妙的。

5、总结
首先我们将内存抽象成一个个页，每个页大小4096字节，然后实现虚拟地址空间与物理地址空间映射时，我们要指定该物理地址映射到哪个虚拟地址，映射大小（必须页对齐），我们使用了三级页表，寻址空间只使用了64位地址的低39位，将其分成四段，前三段每段9位，分别对应每一级页表的index，第4段十二位，表示在实际页中偏移。举个例子：
前面我们实现了0x10000000 映射到 0x10000000，三级页表查询
如果我们现在访问0x10000010，访问过程如下
VA = 0x1000 0010 VA[38:30] = 0 ⇒ 一级页表0x87FFF000[0] = 0x21FFF801,该PTE有效，计算出二级页表的地址
((0x21FFF801 >> 10) << 12) = 0x87FFE000(左移时为了将低10位属性位清零，右移12位是为了保持4096字节对齐) ⇒ VA[29:21] = 0x80 = 128 ⇒ 0x87FFE000[128] = 0x21FFF401,该PTE有效，计算出三级页表的地址
(0x21FFF401 >> 10) << 12) = 0x87FFD000 ⇒ VA[20:12] = 0 ⇒ 0x87FFD000[0] = 0x04000007,该PTE有效，并且该页可读可写，该页的物理地址位 ((0x04000007 >> 10) << 12) = 0x10000000 ⇒ 这里找出了VA对应的页，又页偏移，VA[11:0] = 0x010, 0x10000000 + VA[11:00] = 0x10000010，最终就可以访问到指定的物理地址，就实现了一次虚拟地址到物理地址的转换，然后实际这样的地址转换是不需要我们做的，硬件上的MMU就是做这些事情的。







MySQL 数据库常用命令大全（完整版）
原创
已于 2023-10-14 22:01:11 修改
·
7.8w 阅读
·

273
·
 1.6k
·
CC 4.0 BY-SA版权
文章标签：
#mysql


MySQL
专栏收录该内容
1 篇文章
订阅专栏
本文详细介绍了MySQL的基础命令，包括连接、登录、数据库操作、数据定义语言(DDL)如创建、删除和修改表结构，数据操作语言(DML)如插入、删除和修改数据，数据查询语言(DQL)如查询和筛选数据，以及数据控制语言(DCL)如权限管理。
该文章已生成可运行项目，
预览并下载项目源码
文章目录
1. MySQL命令
2. MySQL基础命令
3. MySQL命令简介
4. MySQL常用命令
4.1 MySQL准备篇
4.1.1 启动和停止MySQL服务
4.1.2 修改MySQL账户密码
4.1.3 MySQL的登陆和退出
4.1.4 查看MySQL版本
4.2 DDL篇（数据定义）
4.2.1 查询数据库
4.2.2 创建数据库
4.2.3 使用数据库
4.2.4 删除数据库
4.2.5 查询表
4.2.6 创建表
4.2.7 修改表
4.2.8 删除表
4.2.9 查看数据表结构
4.2.10 查看建表语句
4.2.11 增加、删除和修改字段自增长
4.2.12 增加、删除和修改数据表的列
4.2.13 添加、删除和查看索引
4.2.14 创建临时表
4.2.15 创建内存表
4.2.16 查看数据库数据表存储位置
4.2.17 清空表内容
4.3 DML篇（数据操作）
4.3.1 数据增加
4.3.2 数据删除
4.3.3 数据修改
4.4 DQL篇（数据查询）
4.4.1 检索所有数据
4.4.2 指定要检索的列
4.4.3 使用WHERE子句来指定条件
4.4.4 使用聚合函数来计算统计数据
4.4.5 使用GROUP BY来分组数据
4.4.6 使用ORDER BY来排序数据
4.5 DCL篇（数据控制）
4.5.1 GRANT命令：授予访问权限
4.5.2 REVOKE命令：撤销访问权限
4.5.3 SET PASSWORD命令：修改用户口令
4.5.4 FLUSH命令：刷新权限
1. MySQL命令
MySQL命令是用于与MySQL数据库进行交互和操作的命令。这些命令可以用于各种操作，包括连接到数据库、选择数据库、创建表、插入数据、查询数据、删除数据等。
2. MySQL基础命令
默认端口号：3306
查看服务器版本：select version(); 或者 cmd命令 mysql -verison
登录数据库：mysql -uroot -p
退出数据库：exit/quit
查看当前系统下的数据库：show databases;
创建数据库：create 库名;
使用数据库：use 库名;
查看表：show tables;
建表：create table 表名 (字段名 + 空格 + 数据类型);
查看表结构：desc 表名;
添加值：insert into 表名 (列名) values (值);
查看表中所有数据：select * from 表名;
查询建表时的结构：show create table 表名;
删除字段中的值：delete from 表名 where 条件;
删除表中的字段：delete from 表名 drop column 字段名; 或alter table 表名 drop 字段名
删除表：drop table 表名;
删除库：drop database 库名;
主键约束：primary key
唯一约束：unique
非空约束：not null
默认约束：default
外键约束：foreign key（外键）references主表（主键）
查看别的数据库的表格：show tables from 表名;
3. MySQL命令简介
MySQL命令是用于与MySQL数据库进行交互和操作的命令。
以下是一些常用的MySQL命令：

mysql：连接MySQL数据库的命令，需要指定用户名和密码。
use：使用某个数据库。
show databases：显示所有数据库。
create database：创建新的数据库。
drop database：删除数据库。
create table：创建新的表。
alter table：修改表结构。
drop table：删除表。
insert into：向表中插入新的记录。
delete from：删除表中符合条件的记录。
update：更新表中符合条件的记录。
select：查询表中的记录。
where：对查询的记录进行条件筛选。
order by：对查询的记录进行排序。
group by：对查询的记录进行分组。
having：对分组后的记录进行筛选。
set names：设置字符集编码。
这些命令只是MySQL命令的一部分，还有更多的命令可以用于管理和操作MySQL数据库。

4. MySQL常用命令
4.1 MySQL准备篇
4.1.1 启动和停止MySQL服务
net start mysql // 启动mysql服务
net stop mysql // 停止mysql服务
运行项目并下载源码
sql
1
2
4.1.2 修改MySQL账户密码
修改MySQL的root用户密码，先登陆MySQL
mysql -u root -p123456
运行项目并下载源码
sql
1
注意：mysql -uroot -p你的MySQL密码

修改root用户的密码：
ALTER USER 'root'@'localhost' IDENTIFIED BY '新密码';
运行项目并下载源码
sql
1
4.1.3 MySQL的登陆和退出
MySQL登陆
win+R 输入cmd，打开命令行窗口，输入mysql -uroot -p123456 ，回车，出现下图且左下角为mysql> ，则登录成功。
mysql -uroot -p123456
运行项目并下载源码
sql
1
注意：mysql -uroot -p你的MySQL密码



MySQL登出
exit
quit
运行项目并下载源码
sql
1
2
二选一

4.1.4 查看MySQL版本
SELECT VERSION();
运行项目并下载源码
sql
1
4.2 DDL篇（数据定义）
在MySQL中，DDL是数据定义语言（Data Definition Language）的缩写，用于定义和管理数据库的结构。
4.2.1 查询数据库
查询所有的数据库
SHOW DATABASES;
运行项目并下载源码
sql
1
4.2.2 创建数据库
创建数据库：
CREATE DATABASE 数据库名称;
运行项目并下载源码
sql
1
创建数据库(判断，如果不存在则创建)
CREATE DATABASE IF NOT EXISTS 数据库名称;
运行项目并下载源码
sql
1
4.2.3 使用数据库
使用数据库
USE 数据库名称;
运行项目并下载源码
sql
1
查看当前使用的数据库
SELECT DATABASE();
运行项目并下载源码
sql
1
4.2.4 删除数据库
删除数据库
DROP DATABASE 数据库名称;
运行项目并下载源码
sql
1
删除数据库(判断，如果存在则删除)
DROP DATABASE IF EXISTS 数据库名称;
运行项目并下载源码
sql
1
4.2.5 查询表
查询当前数据库下所有表名称
SHOW TABLES;
运行项目并下载源码
sql
1
查询表结构
DESC 表名称;
运行项目并下载源码
sql
1
4.2.6 创建表
创建表
CREATE TABLE 表名 (
字段名1 数据类型1,
字段名2 数据类型2,
…
字段名n 数据类型n
);
运行项目并下载源码
sql
1
2
3
4
5
6
create table tb_user (
id int,
username varchar(20),
password varchar(32)
);
运行项目并下载源码
sql
1
2
3
4
5
注意：最后一行末尾，不能加逗号

4.2.7 修改表
修改表名
ALTER TABLE 表名 RENAME TO 新的表名;
-- 将表名student修改为stu
alter table student rename to stu;
运行项目并下载源码
sql
1
2
3
添加一列
ALTER TABLE 表名 ADD 列名 数据类型;
-- 给stu表添加一列address，该字段类型是varchar(50)
alter table stu add address varchar(50);
运行项目并下载源码
sql
1
2
3
修改数据类型
ALTER TABLE 表名 MODIFY 列名 新数据类型;
-- 将stu表中的address字段的类型改为 char(50)
alter table stu modify address char(50);
运行项目并下载源码
sql
1
2
3
修改列名和数据类型
ALTER TABLE 表名 CHANGE 列名 新列名 新数据类型;
-- 将stu表中的address字段名改为 addr，类型改为varchar(50)
alter table stu change address addr varchar(50);
运行项目并下载源码
sql
1
2
3
删除列
ALTER TABLE 表名 DROP 列名;
-- 将stu表中的addr字段 删除
alter table stu drop addr;
运行项目并下载源码
sql
1
2
3
4.2.8 删除表
删除表
DROP TABLE 表名;
运行项目并下载源码
sql
1
删除表时判断表是否存在
DROP TABLE IF EXISTS 表名;
运行项目并下载源码
sql
1
4.2.9 查看数据表结构
desc [表名];
运行项目并下载源码
sql
1
4.2.10 查看建表语句
SHOW CREATE TABLE [表名]
运行项目并下载源码
sql
1
4.2.11 增加、删除和修改字段自增长
（1）增加自增长字段

ALTER TABLE table_name ADD id INT NOT NULL AUTO_INCREMENT PRIMARY KEY;
运行项目并下载源码
sql
1
注意：table_name代表您要增加自增长字段的表名，id代表您要增加的自增长字段名。

（2）修改自增长字段

ALTER TABLE table_name CHANGE column_name new_column_name INT NOT NULL AUTO_INCREMENT PRIMARY KEY;
运行项目并下载源码
sql
1
table_name代表包含自增长字段的表名，column_name代表原始自增长字段名，new_column_name代表新的自增长字段名。请注意，将数据类型更改为INT，否则无法使该列成为自增长主键。完成后，您需要重新启动表格才能使修改生效。
（3）删除自增长字段

ALTER TABLE table_name MODIFY column_name datatype;
运行项目并下载源码
sql
1
注意：table_name代表要删除自增长字段的表名，column_name代表要删除的自增长字段名，datatype代表要设置的数据类型。

4.2.12 增加、删除和修改数据表的列
（1）增加数据表的列

ALTER TABLE <表名> ADD COLUMN <列名> <数据类型>;

ALTER TABLE student ADD COLUMN age INT;
运行项目并下载源码
sql
1
2
3
上面的命令会在student表中增加一个名为age的INT类型列。

（2）删除数据表的列

ALTER TABLE <表名> DROP COLUMN <列名>;

ALTER TABLE student DROP COLUMN age;
运行项目并下载源码
sql
1
2
3
上面的命令会从student表中删除名为age的列。

（3）修改数据表的列

ALTER TABLE <表名> MODIFY COLUMN <列名> <数据类型>;

ALTER TABLE student MODIFY COLUMN age VARCHAR(10);
运行项目并下载源码
sql
1
2
3
上面的命令会将student表中的age列的数据类型修改为VARCHAR(10)。

4.2.13 添加、删除和查看索引
（1）添加索引：

要为表中的某个列添加索引，可以使用以下命令：
ALTER TABLE table_name ADD INDEX index_name (column_name);
运行项目并下载源码
sql
1
其中，table_name是表的名称，index_name是索引的名称，
column_name是要添加索引的列的名称。

例如，如果要为名为users的表的email列添加名为idx_email的索引，可以使用以下命令：
ALTER TABLE users ADD INDEX idx_email (email);
运行项目并下载源码
sql
1
（2）删除索引：

要删除表中的索引，可以使用以下命令：
ALTER TABLE table_name DROP INDEX index_name;
运行项目并下载源码
sql
1
其中，table_name是表的名称，index_name是要删除的索引的名称。

例如，如果要删除名为users的表的idx_email索引，可以使用以下命令：
ALTER TABLE users DROP INDEX idx_email;
运行项目并下载源码
sql
1
（3）查看索引：

要查看表中的索引信息，可以使用以下命令：
SHOW INDEX FROM table_name;
运行项目并下载源码
sql
1
其中，table_name是表的名称。该命令将返回包含索引信息的结果集。

例如，如果要查看名为users的表的索引信息，可以使用以下命令：
SHOW INDEX FROM users;
运行项目并下载源码
sql
1
4.2.14 创建临时表
要创建临时表，可以使用以下语法：
CREATE TEMPORARY TABLE temp_table_name (  
    column1 datatype,  
    column2 datatype,  
    ...  
	);
运行项目并下载源码
sql
1
2
3
4
5
其中，temp_table_name是您要创建的临时表的名称。您可以指定表的列和数据类型，就像创建常规表一样。

以下是一个具体的例子：
CREATE TEMPORARY TABLE temp_users (  
    id INT PRIMARY KEY,  
    name VARCHAR(50),  
    email VARCHAR(100)  
);
运行项目并下载源码
sql
1
2
3
4
5
上述命令将创建一个名为temp_users的临时表，其中包含id、name和email列。id列是主键。

注意：临时表仅在当前会话可见，并且在会话结束时自动删除。因此，它是一种在会话过程中存储临时数据的便捷方式。
4.2.15 创建内存表
要创建内存表，可以使用以下语法：
CREATE TABLE mem_table_name (  
    column1 datatype,  
    column2 datatype,  
    ...  
) ENGINE=MEMORY;
运行项目并下载源码
sql
1
2
3
4
5
其中，mem_table_name是您要创建的内存表的名称。您可以指定表的列和数据类型，就像创建常规表一样。通过将ENGINE选项设置为MEMORY，该表将被创建为内存表。

以下是一个具体的例子：

CREATE TABLE mem_users (  
    id INT PRIMARY KEY,  
    name VARCHAR(50),  
    email VARCHAR(100)  
) ENGINE=MEMORY;
运行项目并下载源码
sql
1
2
3
4
5
上述命令将创建一个名为mem_users的内存表，其中包含id、name和email列。id列是主键。

注意：内存表存储在内存中，因此数据的修改会立即生效，并且对所有用户可见。但是，当MySQL服务器关闭时，内存表中的数据将丢失。因此，它适用于临时存储数据或缓存等场景。
4.2.16 查看数据库数据表存储位置
要查看MySQL数据库中数据表的存储位置，您可以执行以下步骤：

连接到MySQL服务器，可以使用以下命令：
mysql -u username -p
运行项目并下载源码
sql
1
其中，username是您的MySQL用户名。系统将提示您输入密码。

选择要查看存储位置的数据库。使用以下命令选择数据库：

USE database_name;
运行项目并下载源码
sql
1
其中，database_name是您要查看存储位置的数据库的名称。

执行以下命令来查看数据表的存储位置：
SHOW TABLE STATUS;
运行项目并下载源码
sql
1
该命令将返回包含有关数据库中所有数据表的信息的结果集。其中，可以关注File列，它将显示数据表的存储位置。

如果只想查看特定数据表的存储位置，可以结合使用SHOW TABLE
STATUS和LIKE语句。例如，要查看名为table_name的表的存储位置，可以执行以下命令：
SHOW TABLE STATUS LIKE 'table_name';
运行项目并下载源码
sql
1
这将返回特定数据表的详细信息，包括存储位置。

注意：这些命令在MySQL版本5.5及更高版本中有效。
4.2.17 清空表内容
要清空MySQL表的内容，可以使用以下命令：
TRUNCATE TABLE table_name;
运行项目并下载源码
sql
1
其中，table_name是要清空内容的表的名称。

该命令将删除表中的所有数据，但保留表的结构。换句话说，它将删除表中的所有行，但保留表的主键、索引和其他属性。

注意：该操作一旦执行，无法恢复表的内容。因此，在使用该命令之前，请确保您已经备份了重要的数据。
4.3 DML篇（数据操作）
DML是数据操纵语言。它是一组用于管理和处理数据库的命令和语句，用于插入、更新、删除、查询和修改数据库中的数据。

MySQL命令中关于DML的操作主要有增加（Insert）、删除（Delete）和修改（Update）

4.3.1 数据增加
增加操作：
INSERT INTO table_name (column1, column2, column3, ...) VALUES (value1, value2, value3, ...);
运行项目并下载源码
sql
1
例如，要在名为users的表中添加一条记录，可以执行以下命令：

INSERT INTO users (id, name, email) VALUES (1, 'John Doe', 'john@example.com');
运行项目并下载源码
sql
1
4.3.2 数据删除
删除操作：
DELETE FROM table_name WHERE condition;
运行项目并下载源码
sql
1
例如，要删除名为users表中id为1的记录，可以执行以下命令：

DELETE FROM users WHERE id = 1;
运行项目并下载源码
sql
1
4.3.3 数据修改
修改操作：
UPDATE table_name SET column1 = value1, column2 = value2 WHERE condition;
运行项目并下载源码
sql
1
例如，要修改名为users表中name为’John Doe’的记录的email，可以执行以下命令：

UPDATE users SET email = 'newemail@example.com' WHERE name = 'John Doe';
运行项目并下载源码
sql
1
4.4 DQL篇（数据查询）
MySQL中DQL（Data Query Language）操作的主要命令是SELECT，用于从数据库表中检索数据。
4.4.1 检索所有数据
SELECT * FROM table_name;
运行项目并下载源码
sql
1
这个命令将返回表中的所有记录。

4.4.2 指定要检索的列
SELECT column1, column2 FROM table_name;
运行项目并下载源码
sql
1
这个命令将返回指定的列，例如column1和column2。

4.4.3 使用WHERE子句来指定条件
SELECT * FROM table_name WHERE condition;
运行项目并下载源码
sql
1
这个命令将返回满足指定条件的所有记录。例如，SELECT * FROM users WHERE age > 18 将返回年龄大于 18 的所有用户记录。

4.4.4 使用聚合函数来计算统计数据
SELECT COUNT(*) FROM table_name;
运行项目并下载源码
sql
1
这个命令将返回表中的记录数。还可以使用其他聚合函数，如SUM、AVG、MAX和MIN等。

4.4.5 使用GROUP BY来分组数据
SELECT column1, COUNT(*) FROM table_name GROUP BY column1;
运行项目并下载源码
sql
1
这个命令将按column1分组，并计算每个组中的记录数。

4.4.6 使用ORDER BY来排序数据
SELECT * FROM table_name ORDER BY column1;
运行项目并下载源码
sql
1
这个命令将按column1的升序排序所有记录。还可以使用DESC关键字来按降序排序。#### 4.4.7 使用LIMIT来限制返回的记录数

SELECT * FROM table_name LIMIT 10;
运行项目并下载源码
sql
1
这个命令将返回表中的前10条记录。还可以使用OFFSET关键字来指定从哪一行开始返回记录。

4.5 DCL篇（数据控制）
MySQL中DCL（Data Control Language）操作的主要命令是用于管理用户和权限的。
4.5.1 GRANT命令：授予访问权限
GRANT 权限列表 ON 对象类型 对象名称 TO 用户名@用户地址 IDENTIFIED BY用户口令;
运行项目并下载源码
sql
1
例如，给用户test授予对所有数据库的完全访问权限：

GRANT ALL PRIVILEGES ON *.* TO 'test'@'localhost' IDENTIFIED BY 'password';
运行项目并下载源码
sql
1
4.5.2 REVOKE命令：撤销访问权限
REVOKE 权限列表 ON 对象类型 对象名称 FROM 用户名@用户地址;
运行项目并下载源码
sql
1
例如，撤销用户test对所有数据库的访问权限：

REVOKE ALL PRIVILEGES ON *.* FROM 'test'@'localhost';
运行项目并下载源码
sql
1
4.5.3 SET PASSWORD命令：修改用户口令
SET PASSWORD FOR 用户名@用户地址 = SET PASSWORD BY PASSWORD ('新口令');
运行项目并下载源码
sql
1
例如，将用户test的口令修改为新口令：

SET PASSWORD FOR 'test'@'localhost'=SET PASSWORD BY PASSWORD ('newpassword');
运行项目并下载源码
sql
1
4.5.4 FLUSH命令：刷新权限
FLUSH PRIVILEGES;
运行项目并下载源码
sql
1
例如，刷新权限使应用立即更改：







JDBC数据库连接 及JDBC使用讲解
原创
于 2022-03-13 10:35:47 发布
·
2.3k 阅读
·

2
·
 4
·
CC 4.0 BY-SA版权
文章标签：
#数据库
#database


mysql
专栏收录该内容
16 篇文章
订阅专栏
本文介绍了JDBC的基本概念、工作原理以及在Java项目中如何通过DataSource操作MySQL数据库，包括添加驱动、连接数据库、执行SQL语句和关闭连接。重点讲解了PreparedStatement的使用及其优势。
目录

1. 什么是JDBC

2. JDBC ⼯作原理

3. JDBC 使⽤

3.1 创建项目并添加MySQL驱动(DataSource实现操作数据库)

3.2 使用代码操作数据库

1. 什么是JDBC
Java 数据库连接。是⼀种⽤于执⾏ SQL 语句的 Java API，它是 Java 中的数据库连接规范。 使⽤了 JDBC 之后，不管是什么数据库与什么数据库驱动，我们只需要使⽤⼀套标准代码就 可以实现对不同数据库进⾏统⼀操作（添加、修改、删除、查询），它的目的就在于解决不同数据库厂商的数据库存在不一样时，会产生的问题。
2. JDBC ⼯作原理
JDBC 为多种关系数据库提供了统⼀访问⽅式，作为特定⼚商数据库访问 API 的⼀种⾼级抽象，
它主要包含⼀些通⽤的接⼝类。
JDBC 访问数据库层次结构：

JDBC的优点：1. Java语言访问数据库操作完全面向抽象接口编程。

                        2. 开发数据库应用不用限定在特定数据库厂商的API。
    
                        3. 程序的可移植性大大增强。 

3. JDBC 使⽤
   3.1 创建项目并添加MySQL驱动(DataSource实现操作数据库)
        创建Java项目，添加MySQL驱动（注意：不同数据库版本对应不同驱动包）

3.2 使用代码操作数据库
        操作数据库MySQL提供两种操作API：

        1. DirverManager
    
        2. DataSource(推荐使用)

下面是使用DataSource来实现操作数据库：

        1. 获取数据源   （输入MySQL数据库连接的服务器地址、用户名、密码）
    
        2. 获取连接  （建立客户端与服务端的连接）
    
        3. 获取执行器  （连接服务器并切换到数据库）
    
        4. 查询或操作数据库  （输入命令，并得到结果）
    
        5. 关闭连接  （关闭客户端）

3.2.1 获取数据源

// 1. 创建数据源
MysqlDataSource dataSource = new MysqlDataSource();
// 1.1 设置连接的 MySQL 数据库服务器
dataSource.setURL("jdbc:mysql://127.0.0.1:3306/java33characterEncoding=utf8&useSSL=true");
// 1.2 设置⽤户名
dataSource.setUser("root");
// 1.3 设置密码
dataSource.setPassword("********");
AI写代码
sql

 3.2.2  获得连接

连接对象是 Connection，注意此步骤操作容易出错，⼀定是 com.mysql.jdbc 包下的 Connection 对象。
// 2.创建连接
Connection connection = (Connection) dataSource.getConnection();
AI写代码
sql
3.2.3 获得执行器

执⾏器是⽤来执⾏ SQL 命令的，执⾏器有三种：
        1. Statement （用于执行不带参数的简单语句）
        2. PreparedStatement （用于执行带或者不带参数的语句，SQL语句会在数据库系统预编译，执行速度快于Statement）（常用）
        3. CallableStatement （用于执行数据库存储过程的调用）
PreparedStatement 优点：
        性能比 Statement高，SQL预编译，阻止常见SQL注入攻击，占位符“ ？”下标从1开始，占位符不能用多值，参数化SQL查询。
// 3.获得执⾏器
String selectSQL = "select * from student where user name = ?";
PreparedStatement statement = connection.prepareStatement(selectSQL);
// 3.1 占位符赋值
statement.setString(1, "王五");
AI写代码
sql

PreparedStatement 有 主要两种重要的⽅法：
        1. executeQuery();  ⽅法执⾏后返回单个结果集的，通常⽤于 select 语句。
                2. executeUpdate()：⽅法返回值是⼀个整数，指示受影响的⾏数，通常⽤于 update、insert、delete 语句。
        3.2.4 查询或操作数据库
        // 4.获得结果集
        ResultSet resultSet = statement.executeQuery();
        // 4.1 获得结果并打印
        while (resultSet.next()) {
        String sn = resultSet.getString("sn");
        String username = resultSet.getString("username");
        String mail = resultSet.getString("mail");
        System.out.println(String.format("SN：%s，UserName：%s，Mail：%s", sn,username, mail));
        }
        AI写代码
        sql
        ResultSet ⾥的数据⼀⾏⼀⾏排列，每⾏有多个字段，并且有⼀个记录指针，指针所指的数据⾏
        叫做当前数据⾏，我们只能来操作当前的数据⾏。我们如果想要取得某⼀条记录，就要使⽤
        ResultSet 的 next() ⽅法 ,如果我们想要得到 ResultSet ⾥的所有记录，就应该使⽤ while 循环。
        3.2.5 关闭数据库连接
    // 5. 关闭资源
                resultSet.close();
                statement.close();
                connection.close();









java AWT布局管理器总结
原创
已于 2022-10-09 22:12:24 修改
·
1.2k 阅读
·

1
·
 5
·
CC 4.0 BY-SA版权
文章标签：
#java
#jvm
#开发语言


java
专栏收录该内容
29 篇文章
订阅专栏
本文介绍了Java中GUI设计的布局管理器，包括FlowLayout、BorderLayout、GridLayout和CardLayout的特点及使用方法。
GUI全称是Graphical User Interface，即图形用户界面。
Java中针对GUI设计提供了丰富的类库，这些类分别位于java.awt和javax.swing包中，简称为AWT和Swing。

AWT是用于创建图形用户界面的一个工具包，它提供了一系列用于实现图形界面的组件，如窗口、按钮、文本框、对话框等。在JDK中针对每个组件都提供了对应的Java类，这些类都位于java.awt包中，接下来通过一个图例来描述这
些类的继承关系，如图所示。

从图的继承关系可以看出，在AWT中组件分为两大类，这两类的基类分别是Component和MenuComponent。其中，MenuComponent是所有与菜单相关组件的父类，Component则是除菜单外其他AWT组件的父类，它表示一个能
以图形化方式显示出来，并可与用户交互的对象。

Component类通常被称为组件，根据Component的不同作用，可将其分为基本组件类和容器类。基本组件类是诸如按钮、文本框之类的图形界面元素，而容器类则是通过Component的子类Container实例化的对象。Container类表示容器，它是一种特殊的组件，可以用来容纳其他组件。Container容器又分为两种类型，分别是Window和Panel

Window类是不依赖其他容器而独立存在的容器，它有两个子类，分别是Frame类和Dialog类。Frame类用于创建一个具有标题栏的框架窗口，作为程序的主界面，Dialog类用于创建一个对话框，实现与用户的信息交互

Panel也是一个容器，但是它不能单独存在，只能存在其他容器（Window或其子类）中，一个Panel对象代表了一个长方形的区域，在这个区域中可以容纳其他组件。在程序中通常会使用Panel来实现一些特殊的布局。

import java.awt.*;

public class FrameEx {
    public static void main(String[] args) {
        Frame frame=new Frame("我的窗口");
        frame.setSize(800,300);
        frame.setLocation(600,200);
        frame.setVisible(true);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
组件不能单独存在，必须放置于容器当中，而组件在容器中的位置和尺寸是由布局管理器来决定的。

在java.awt包中提供了多种布局管理器，包括FlowLayout（流式布局管理器）、BorderLayout（边界布局管理器）、GridLayout（网格布局管理器）和CardLayout（卡片布局管理器）。每个容器在创建时都会使用一种默认的布局管理器，在程序中可以通过调用容器对象的setLayout()方法设置布局管理器，通过布局管理器来自动进行组件的布局管理。

例如把一个Frame窗体的布局管理器设置为FlowLayout，代码如下所示：

Frame frame = new Frame();
frame.setlayout(new FlowLayout());

流式布局管理器（FlowLayout）
最简单的布局管理器，在这种布局下，容器会将组件按照添加顺序从左向右放置。当到达容器的边界时，会自动将组件放到下一行的开始位置。这些组件可以左对齐、居中对齐（默认方式）或右对齐的方式排列。FlowLayout对象有三个构造方法，如表所示。

表中，列出了FlowLayout的三个构造方法，其中，参数align决定组件在每行中相对于容器边界的对齐方式，可以使用该类中提供的常量作为参数传递给构造方法，其中FlowLayout.LEFT用于表示左对齐、FlowLayout.RIGHT用于表示右对齐、FlowLayout.CENTER用于表示居中对齐。参数hgap和参数vgap分别设定组件之间的水平和垂直间隙，可以填入一个任意数值。

import java.awt.*;

public class FlowEx {
    public static void main(String[] args) {
        final Frame f=new Frame("flowlayout");
        f.setLayout(new FlowLayout(FlowLayout.LEFT,20,30));
        f.setSize(220,300);
        f.setLocation(300,200);
        f.setVisible(true);
        f.add(new Button("第1个按钮"));
        f.add(new Button("第2个按钮"));
        f.add(new Button("第3个按钮"));
        f.add(new Button("第4个按钮"));
        f.add(new Button("第5个按钮"));
        f.add(new Button("第6个按钮"));
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17


BorderLayout（边界布局管理器）
一种较为复杂的布局方式，它将容器划分为五个区域，分别是东(EAST)、南(SOUTH)、西(WEST)、北(NORTH)、中(CENTER)。组件可以被放置在这五个区域中的任意一个。BorderLayout布局的效果如图所示。

从图可以看出BorderLayout边界布局管理器，将容器划分为五个区域，其中箭头是指改变容器大小时，各个区域需要改变的方向。也就是说，在改变容器时NORTH和SOUTH区域高度不变长度调整，WEST和EAST区域宽度不变高度调整，CENTER会相应进行调整。

当向BorderLayout布局管理器的容器中添加组件时，需要使用add(Component comp,Object constraints)方法。其中参数comp表示要添加
的组件，constraints指定将组件添加到布局中的方式和位置的对象，它是一个Object类型，在传参时可以使用BorderLayout类提供的5个常量，它们分别是EAST、SOUTH、WEST、NORTH和CENTER。

import java.awt.*;

public class BorderEx {
    public static void main(String[] args) {
        final Frame f=new Frame("border layout");
        f.setLayout(new BorderLayout());
        f.setSize(300,300);
        f.setLocation(300,300);
        f.setVisible(true);
        f.add(new Button("东部"),BorderLayout.EAST);
        f.add(new Button("西部"),BorderLayout.WEST);
        f.add(new Button("北部"),BorderLayout.NORTH);
        f.add(new Button("南部"),BorderLayout.SOUTH);
        f.add(new Button("中部"), BorderLayout.CENTER);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16


GridLayout（网格布局管理器）
使用纵横线将容器分成n行m列大小相等的网格，每个网格中放置一个组件。添加到容器中的组件首先放置在第1行第1列（左上角）的网格中，然后在第1行的网格中从左向右依次放置其他组件，行满后，继续在下一行中从左到右放置组件。与FlowLayout不同的是，放置在
GridLayout布局管理器中的组件将自动占据网格的整个区域。

GridLayout的构造方法，如表所示。

表中，列出了GridLayout的三个构造方法，其中，参数rows代表行数，cols代表列数，hgap和vgap规定水平和垂直方向的间隙。水平间隙指的是网格之间的水平距离，垂直间隙指的是网格之间的垂直距离。

import java.awt.*;

public class GridEx {
    public static void main(String[] args) {
        Frame f=new Frame("grid layout");
        f.setLayout(new GridLayout(3,3));
        f.setSize(300,300);
        f.setLocation(300,300);
        for(int i=0;i<6;i++){
            f.add(new Button("byn"+i));
        }
        f.setVisible(true);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14


卡片布局管理器(CardLayout)
在操作程序时，经常会遇到通过选项卡按钮来切换程序中的界面，这些界面就相当于一张张卡片，而管理这些卡片的布局管理器就是卡片布局管理器(CardLayout)。卡片布局管理器将界面看做是一系列卡片，在任何时候只有其中一张卡片是可见的，这张卡片占据容器的整个区域。
在CardLayout布局管理中经常会用到下面几个方法，如表所示。


import java.awt.*;
import java.awt.event.ActionEvent;
import java.awt.event.ActionListener;

public class CardEx {
    public static void main(String[] args) {
        Frame frame=new Frame();
        frame.setSize(300,300);
        frame.setVisible(true);

        CardLayout cardLayout=new CardLayout();
        Panel cardPanel=new Panel();
        cardPanel.setLayout(cardLayout);
        cardPanel.add(new Label("第一个界面",Label.CENTER));
        cardPanel.add(new Label("第二个界面"),Label.CENTER);
        cardPanel.add(new Label("第三个界面"),Label.CENTER);
    
        Panel controlPanel=new Panel();
        Button nextButton=new Button("下一张卡片");
        Button preButton=new Button("上一张卡片");
        controlPanel.add(nextButton);
        controlPanel.add(preButton);
    
        frame.add(cardPanel,BorderLayout.NORTH);
        frame.add(controlPanel,BorderLayout.SOUTH);



        nextButton.addActionListener(new ActionListener() {
            @Override
            public void actionPerformed(ActionEvent e) {
                cardLayout.next(cardPanel);
            }
        });
    
        preButton.addActionListener(new ActionListener() {
            @Override
            public void actionPerformed(ActionEvent e) {
                cardLayout.previous(cardPanel);
            }
        });


    }
}

AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46

当一个容器被创建后，它们都会有一个默认的布局管理器。Window、Frame和Dialog的默认布局管理器是BorderLayout，Panel的默认布局管理器是FlowLayout。如果不希望通过布局管理器来对容器进行布局，也可以调用容器的setLayout(null)方法，将布局管理器取消。在这种情况下，程序必须调用容器中每个组件的setSize()和setLocation()方法或者是setBounds()方法(这个方法接收四个参数，分别是左上角的x、y坐标和组件的长、宽)来为这些组件在容器中定位。

import java.awt.*;

public class NoLayoutEx {
    public static void main(String[] args) {
        Frame frame=new Frame("hello world");
        frame.setLayout(null);
        frame.setSize(300,300);

        Button btn1=new Button("press");
        Button btn2=new Button("pop");
    
        btn1.setBounds(40,60,100,30);
        btn2.setBounds(140,90,100,30);
    
        frame.add(btn1);
        frame.add(btn2);
    
        frame.setVisible(true);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20











较详细从头到尾配置openpose+stgcn对自建行为数据集模型训练
原创
于 2025-09-25 14:16:21 发布
·
公开
·
762 阅读
·

20
·
 23
·
CC 4.0 BY-SA版权
编辑
文章标签：
#python
#开发语言
#计算机视觉

Python3.8
一键部署
Python 是一种高级、解释型、通用的编程语言，以其简洁易读的语法而闻名，适用于广泛的应用，包括Web开发、数据分析、人工智能和自动化脚本


前提提示：本人很菜，有很多地方其实自己也不知道为什么这样做。写这个目的是让以后自己需要复刻这套流程的话，可以翻出来看看。

vs2019+python3.8+win10+cuda11.8+vscode+cudnn11.8

一、配openpose

这里我几乎是按照这一篇blog进行配置的，很流畅，缺啥配啥。

openpose【超详细】从0开始，环境搭建及应用（GPU）windows10+（conda）python3.8+CUDA11.1+vs2019

里面给出了openpose_py3.8完整的内容，也就不需要去下载那几个.bat文件了。

但是我看了很多篇文章，在vs2019里打开openpose时，是选择Release而不是Debug。

所以可能要选择Release模式。

在这之后呢，我又去看其他blog，就是需要自行下载几个.bat文件的，又遇到几个问题：

        ①下载获取链接失败
    
        ②通过迅雷下载也失败

所以，这里有一篇blog给出了已经下载好的文件内容，openpose安装教程（win10的（一），但是我自己怎么再次重新配置，都配不好。最后灰溜溜进回收站把配好的openpose_py3.8生成的OpenposeDemo项目文件夹给恢复了，并且直接在这基础上继续下一步了。

如果后面还有时间去鼓捣的话，我试试再自己从头到尾配一遍。

二、提取骨格点数据，处理骨骼点数据

1.使用其他数据集先行尝试。

这里我读了这篇blog利用openpose提取自建数据集骨骼点训练st-gcn，复现st-gcn，按照里面的1获取尝试先训练training_lib_KTH.zip。

2.测试OpenposeDemo的使用

有很多blog给出了全部python代码，但是我自己使用的时候路径问题总是搞不好。而且个人习惯是把步骤拆成一部分一部分去做，所以自己用AI+喂各个blog代码写了获取骨格点的代码。

首先是测试OpenposeDemo的使用。在“配好的openpose_py3.8生成的OpenposeDemo项目文件夹”下的bin文件夹里新建一个脚本，



具体代码如下：

注意 params["model_folder"] = "D:/Class_datasets/OpenposeDemo/models/" 双引号内直接改为自己Demo的models的绝对路径。原先看到的文章这里是相对路径，但是我后面运行有报错，所以直接一步到位。

import os
import sys
import cv2
from sys import platform
import argparse

dir_path = os.path.dirname(os.path.realpath(__file__))
if dir_path not in sys.path:
    sys.path.append(dir_path)

os.environ['PATH'] = os.environ['PATH'] + ';' + dir_path + '/bin;'
import pyopenpose as op

print(op)
print("成功引入pyopenpose")

import json

def extract_pose_from_video(video_path, output_dir):
    """
    使用 OpenPose 提取视频逐帧人体关键点，并保存为 JSON 文件。
    Args:
        video_path (str): 输入视频路径
        output_dir (str): 输出 JSON 文件夹路径
        model_dir (str): OpenPose 模型文件夹路径（默认 "models/"）
    """
    # 确保输出目录存在
    os.makedirs(output_dir, exist_ok=True)

    # OpenPose 参数配置
    params = dict()
    params["model_folder"] = "D:/Class_datasets/OpenposeDemo/models/" 
    #这里改为自己的models的绝对路径
    params["net_resolution"] = "368x256"   # 可调节
     
    # 初始化 OpenPose
    opWrapper = op.WrapperPython()
    opWrapper.configure(params)
    opWrapper.start()
     
    # 打开视频
    cap = cv2.VideoCapture(video_path)
    frame_id = 0
     
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
     
        datum = op.Datum()
        datum.cvInputData = frame
        opWrapper.emplaceAndPop(op.VectorDatum([datum]))
     
        # 获取关键点数据
        keypoints = datum.poseKeypoints  # shape: [人数, 25, 3]
        if keypoints is None:
            keypoints_to_save = []
        else:
            keypoints_to_save = keypoints.tolist()
        # 保存为 JSON
        json_path = os.path.join(output_dir, f"frame_{frame_id:06d}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(keypoints_to_save, f)
     
        frame_id += 1
     
    cap.release()
    print(f"完成！关键点 JSON 文件保存在: {output_dir}")

 

# 使用示例：
if __name__ == "__main__":
    extract_pose_from_video(
        video_path="D:/Class_datasets/80B01_23.mp4",
        output_dir="D:/Class_datasets/openpose_json"
    )
AI写代码
python
运行

        video_path="D:/Class_datasets/80B01_23.mp4",
        output_dir="D:/Class_datasets/openpose_json"

这里可以直接改为自己的一个测试视频的绝对路径，以及输出json文件的绝对路径。

3.将视频resized，修改帧率 + 用openpose获取逐帧骨格数据点

我仿照这篇文章里的代码，修改了一下，编写preprocess.py。利用openpose提取自建数据集骨骼点训练st-gcn，复现st-gcn

#!/usr/bin/env python
import os
import argparse
import json
import shutil

import numpy as np
import torch
import skvideo.io

#from processor.io import IO
import stgcn.tools
import stgcn.tools.utils as utils

import sys
sys.path.append("D:/Class_datasets") 
# OpenposeDemo所放置的目录(我的OpensposeDemo放置在Class_datasets文件夹下)

from OpenposeDemo.bin.openpose_extractor import extract_pose_from_video

class PreProcess():
    def start(self, flag1, flag2):

        work_dir = 'D:/VScode/ST-GCN'#项目总目录
        type_number = 6
        action_filename_list = ['boxing', 'handclapping', 'handwaving', 'jogging', 'running', 'walking']
     
        for process_index in range(type_number):
            #process_index 分类下标
     
            action_filename = action_filename_list[process_index] # 标签内容
            # 标签信息
            labelAction_name = '{}_{}'.format(action_filename,process_index)
            label_no = process_index
     
            # 视频所在文件夹
            originvideo_file = 'D:/Class_datasets/dataSet/training_lib_KTH_cut_6s/{}/'.format(action_filename)
            # resized视频输出文件夹 需要自己创建几个分类的文件夹
            # 这里将originvideo_file修改为自己的训练集绝对路径
            resizedvideo_file = 'D:/Class_datasets/dataSet/resized/{}/'.format(action_filename)
            # 这里我新建了resized文件夹, 将修改的视频存放在这里
            videos_file_names = os.listdir(originvideo_file)
     
            # 1. Resize文件夹下的视频到340x256 30fps
            if (flag1):
                for file_name in videos_file_names:
                    video_path = '{}{}'.format(originvideo_file, file_name)
                    outvideo_path = '{}{}'.format(resizedvideo_file, file_name)
                    try:
                        writer = skvideo.io.FFmpegWriter(outvideo_path,
                                                            outputdict={'-f': 'mp4','-vcodec': 'libx264', '-s': '340x256',
                                                                        '-r': '30'})
                        reader = skvideo.io.FFmpegReader(video_path)
                        for frame in reader.nextFrame():
                            writer.writeFrame(frame)
                        writer.close()
                        print('{} resize success'.format(file_name))
                    except Exception as e:
                        print('{} resize error'.format(video_path))
     
            # 2. 利用openpose提取每段视频骨骼点数据
            if (flag2):
     
                # 取目录下的所有视频文件
                resizedvideos_file_names = os.listdir(resizedvideo_file)
     
                for file_name in resizedvideos_file_names:
                    # 拼接视频路径
                    outvideo_path = os.path.join(resizedvideo_file, file_name)
     
                    # 取视频名（不带扩展名）
                    video_name = file_name.split('.')[0]
     
                    # 输出关键点 JSON 的目录（逐帧）
                    output_snippets_dir = f'D:/Class_datasets/dataSet/openpose_json/{video_name}'
     
                    # 删除旧的 JSON 目录（如果有的话），并新建
                    shutil.rmtree(output_snippets_dir, ignore_errors=True)
                    os.makedirs(output_snippets_dir, exist_ok=True)
     
                    print(f"正在处理视频: {file_name}")
                    print(f"输出逐帧 JSON 目录: {output_snippets_dir}")
     
                    # 调用 openpose_extractor.py 的函数，提取逐帧关键点
                    extract_pose_from_video(
                        video_path=outvideo_path,
                        output_dir=output_snippets_dir,
                    )

if __name__ == '__main__':
    p=PreProcess()
    p.start(0, 1)
AI写代码
python
运行

resized后的文件示例如下：





获取视频帧骨骼数据如下：





这里获取骨骼关键点耗时很长，我使用了一个GPU的情况下跑了14个小时左右。

4.将获取的逐帧骨骼数据点进行合并

也就是把例如boxing的person01一共240左右个json文件合并为一个person01_boxing_....的json文件，代码如下，我自己命名为packing.py

import os
import json
import re
from glob import glob

# 输入和输出路径
OPENPOSE_JSON_ROOT = "D:/Class_datasets/dataSet/openpose_json"
# 我这里输入和输出路径一致 只是存放在其他文件夹里 
# 如果需要 可自行改为输入输出各一个路径

# 预定义类别索引（可按需调整）
CLASS_LABELS = {
    "boxing": 0,
    "handclapping": 1,
    "handwaving": 2,
    "jogging": 3,
    "running": 4,
    "walking": 5
}

def extract_class_from_name(folder_name):
    """根据文件夹名提取类别"""
    parts = folder_name.split("_")
    if len(parts) >= 2:
        return parts[1]
    else:
        return "unknown"

def load_keypoints(json_path):
    """读取单帧的OpenPose JSON文件（已转为简化数组格式）"""
    with open(json_path, "r") as f:
        data = json.load(f)

    # 如果数据为空，返回 []
    if not data or not isinstance(data, list):
        return []
     
    # OpenPose Demo中，单帧JSON是三层嵌套 [[[x,y,conf],...]]
    keypoints = data[0]  # shape: (25, 3)
     
    pose = []
    score = []
    for x, y, c in keypoints:
        pose.append([x, y, c])
        score.append(c)
     
    return [{
        "pose": pose,
        "score": score
    }]

def pack_folder(folder_path, class_name, label_index, output_dir):
    """将单个视频的逐帧JSON打包为ST-GCN格式"""
    folder_name = os.path.basename(folder_path)
    output_path = os.path.join(output_dir, f"{folder_name}.json")

    frame_files = sorted(glob(os.path.join(folder_path, "*.json")))
     
    data = []
    for idx, frame_path in enumerate(frame_files, 1):
        skeleton = load_keypoints(frame_path)
        frame_dict = {
            "frame_index": idx,
            "skeleton": skeleton
        }
        data.append(frame_dict)
     
    packed = {
        "data": data,
        "label": class_name,
        "label_index": label_index
    }
     
    os.makedirs(output_dir, exist_ok=True)
    with open(output_path, "w") as f:
        json.dump(packed, f)
     
    print(f"打包完成: {output_path}")

def main():
    video_folders = [f for f in os.listdir(OPENPOSE_JSON_ROOT) 
                     if os.path.isdir(os.path.join(OPENPOSE_JSON_ROOT, f))]

    for folder_name in video_folders:
        class_name = extract_class_from_name(folder_name)
        if class_name not in CLASS_LABELS:
            print(f"跳过未知类别: {folder_name}")
            continue
     
        label_index = CLASS_LABELS[class_name]
        folder_path = os.path.join(OPENPOSE_JSON_ROOT, folder_name)# 输入
        output_dir = os.path.join(OPENPOSE_JSON_ROOT, class_name)# 输出
     
        pack_folder(folder_path, class_name, label_index, output_dir)

if __name__ == "__main__":
    main()
AI写代码
python
运行

代码运行完成后示例如下：


打开boxing，示例如下：


其实这里文件放置安排的不是很好，但是代码能跑，我自己就懒得再改了。

5.再将json文件按比例分配给train，test，val数据集。

基本也是按照利用openpose提取自建数据集骨骼点训练st-gcn，复现st-gcn里的数据集划分步骤进行的。我AI了一个小脚本进行数据集划分。划分后文件夹示例如下：



allocate.py代码如下：

import os
import re
import shutil
import sys

# --- 配置（如需改动请在此修改） ---
SOURCE_ROOT = r"D:\Class_datasets\dataSet\openpose_json"# 已经合并好的文件所在绝对路径
TARGET_ROOT = r"D:\Class_datasets\dataSet\kinetics-skeleton"# json文件复制粘贴的输出绝对路径
TRAIN_DIR = os.path.join(TARGET_ROOT, "kinetics_train")
VAL_DIR = os.path.join(TARGET_ROOT, "kinetics_val")
TEST_DIR = os.path.join(TARGET_ROOT, "kinetics_test")
# ------------------------------------

PERSON_RE = re.compile(r"person0*([0-9]{1,2})", re.IGNORECASE)  # 捕获 person01 或 person1 等

def ensure_dirs():
    """确保目标目录存在"""
    for d in (TRAIN_DIR, VAL_DIR, TEST_DIR):
        os.makedirs(d, exist_ok=True)

def decide_target(person_idx):
    """根据 person 编号返回目标目录路径，编号为 int"""
    if 1 <= person_idx <= 15:
        return TRAIN_DIR
    elif 16 <= person_idx <= 20:
        return VAL_DIR
    elif 21 <= person_idx <= 25:
        return TEST_DIR
    else:
        return None

def allocate():
    ensure_dirs()
    stats = {"train": 0, "val": 0, "test": 0, "skipped": 0, "errors": 0}
    skipped_files = []
    error_files = []

    # 遍历 source root 下的一级目录（假定为类别文件夹）
    if not os.path.isdir(SOURCE_ROOT):
        print(f"错误：源目录不存在: {SOURCE_ROOT}")
        sys.exit(1)
     
    for item in os.listdir(SOURCE_ROOT):
        item_path = os.path.join(SOURCE_ROOT, item)
        if not os.path.isdir(item_path):
            # 跳过非目录项（文件等）
            continue
     
        # 遍历该类别目录下的文件
        for fname in os.listdir(item_path):
            if not fname.lower().endswith(".json"):
                continue
     
            src_path = os.path.join(item_path, fname)
            # 抽取 person 编号
            m = PERSON_RE.search(fname)
            if not m:
                stats["skipped"] += 1
                skipped_files.append((src_path, "no_person_pattern"))
                continue
     
            try:
                person_idx = int(m.group(1))
            except ValueError:
                stats["skipped"] += 1
                skipped_files.append((src_path, f"bad_person_number:{m.group(1)}"))
                continue
     
            target_dir = decide_target(person_idx)
            if target_dir is None:
                stats["skipped"] += 1
                skipped_files.append((src_path, f"person_out_of_range:{person_idx}"))
                continue
     
            dst_path = os.path.join(target_dir, fname)
            try:
                # 使用 copy2 保留元信息；若目标存在则直接覆盖
                shutil.copy2(src_path, dst_path)
                # 统计
                if target_dir == TRAIN_DIR:
                    stats["train"] += 1
                elif target_dir == VAL_DIR:
                    stats["val"] += 1
                elif target_dir == TEST_DIR:
                    stats["test"] += 1
            except Exception as e:
                stats["errors"] += 1
                error_files.append((src_path, str(e)))
                print(f"[ERROR] 复制失败: {src_path} -> {dst_path}  错误: {e}")
     
    # 打印汇总
    print("\n===== 分配完成 =====")
    print(f"训练集 (kinetics_train) 复制数: {stats['train']}")
    print(f"验证集 (kinetics_val)   复制数: {stats['val']}")
    print(f"测试集 (kinetics_test)  复制数: {stats['test']}")
    print(f"跳过 (pattern/mismatch) 数: {stats['skipped']}")
    print(f"错误 (copy errors)     数: {stats['errors']}")

 

if __name__ == "__main__":
    allocate()
AI写代码
python
运行

6.编写脚本获得label.json文件

需要获取如下所示的kinetics_test_label.json/kinetics_train_label.json/kinetics_val_label.json文件。



代码如下：

import os
import json

# 骨骼数据点根目录 按照需要获取实际的文件绝对路径
root_dir = r"D:/Class_datasets/dataSet/kinetics-skeleton"

# 三个子文件夹
splits = ["kinetics_train", "kinetics_val", "kinetics_test"]

for split in splits:
    input_dir = os.path.join(root_dir, split)
    output_path = os.path.join(root_dir, f"{split}_label.json")

    label_dict = {}
     
    # 遍历该 split 文件夹下所有 json 文件
    for file_name in os.listdir(input_dir):
        if not file_name.endswith(".json"):
            continue
     
        file_path = os.path.join(input_dir, file_name)
        file_id = os.path.splitext(file_name)[0]  # 去掉扩展名作为键名
     
        with open(file_path, "r") as f:
            data = json.load(f)
     
        # 从 ST-GCN json 文件中提取信息
        label = data.get("label", "")
        label_index = data.get("label_index", -1)
     
        label_dict[file_id] = {
            "has_skeleton": True,
            "label": label,
            "label_index": label_index
        }
     
    # 写入输出文件
    with open(output_path, "w") as f:
        json.dump(label_dict, f)
     
    print(f"{output_path} 已生成，共 {len(label_dict)} 个样本。")
AI写代码
python
运行

三、配置st-gcn

1.下载

https://gitee.com/chenhongqiong/st-gcn或者https://github.com/yysijie/st-gcn，直接下载整个zip，解压即可。

2.环境配置

参考st-gcn训练自建行为识别数据集的第一点，VSC打开stgcn，终端窗口输入命令。

pip install -r requirements.txt
cd torchlight
python setup.py install
AI写代码
python
运行


3.json格式数据转化

需要将json格式转化为stgcn训练需要的npy和pkl格式文件，也是参照该文章的第三点，在stgcn下找到tools/kinetics_gendata.py，如图所示：



修改其中的约35行的信息：

        num_person_in=1,  #observe the first 5 persons 
        num_person_out=1,  #then choose 2 persons with the highest score 
        max_frame=300):
AI写代码
python
运行
max_frame取大于视频的总帧数再多一点，我这里300＞240，差不多。

55行左右的关键点个数：

        shape=(len(sample_name), 3, max_frame, 25, num_person_out))
AI写代码
python
运行
这里初始关键点个数好像是18，我改为25.关键点个数可以通过之前的帧骨骼数据获取。

70~80行左右的路径读取：

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Kinetics-skeleton Data Converter.')
    parser.add_argument(
        '--data_path', default='D:/Class_datasets/dataSet/kinetics-skeleton')
    # default内改为读取的文件绝对路径
    parser.add_argument(
        '--out_folder', default='D:/Class_datasets/dataSet/kinetics-skeleton')
    # default内改为输出的文件绝对路径
    arg = parser.parse_args()

    part = ['train', 'val', 'test'] # 如果缺少'val'，则补上
AI写代码
python
运行

feeder/feeder_kinetics.py下的约83~89行修改：

        # output data shape (N, C, T, V, M)
        self.N = len(self.sample_name)  #sample
        self.C = 3  #channel
        self.T = 300  #frame
        self.V = 25  #joint
        self.M = self.num_person_out  #person
AI写代码
python
运行
与前面修改的一致。

之后运行kinetics_gendata.py，得到转化后的文件：



4.添加layout

同样参考文章的第四点，因为我的关键点数是25个，openpose里的BODY_25，所以需要自行增加。

找到net/utils/graph.py，我将其命名为openpose25，添加在约70行之后：

这里的点关联是通过openpose里的"输入输出文件"的图片展示BODY_25得到的。

        elif layout == 'openpose25':
            self.num_node = 25
            self_link = [(i, i) for i in range(self.num_node)]
            neighbor_1base = [(1, 2), (2, 3), (3, 4), (4, 5), (2, 6), (6, 7), (7, 8),
                              (2, 9), (9, 10), (10, 11), (11, 12), (9, 13), (13, 14), (14, 15),
                              (12, 23), (12, 25), (23, 24), (14, 22), (15, 20), (20, 2),
                              (1, 16), (1, 17), (16, 18), (17, 19)]
            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]
            self.edge = self_link + neighbor_link
            self.center = 1 - 1
        # elif layout=='customer settings'
        #     pass
AI写代码
python
运行

5.修改train.yaml

同样，学习文章第五点。我直接上代码了：

work_dir: ./work_dir/recognition/kinetics_skeleton/ST_GCN

# feeder
feeder: feeder.feeder.Feeder
train_feeder_args:
  random_choose: True
  random_move: True
  # ---------------------- #
  window_size: 300 # 与 kinetics_gendata.py里的max_frame保持一致
  # ---------------------- #
  # window_size: 150 
  # ---------------------------- #
  data_path: D:/Class_datasets/dataSet/kinetics-skeleton/train_data.npy
  label_path: D:/Class_datasets/dataSet/kinetics-skeleton/train_label.pkl
  # ---------------------------- #
  # data_path: ./data/Kinetics/kinetics-skeleton/train_data.npy
  # label_path: ./data/Kinetics/kinetics-skeleton/train_label.pkl
test_feeder_args:
  # ---------------------------- #
  data_path: D:/Class_datasets/dataSet/kinetics-skeleton/val_data.npy
  label_path: D:/Class_datasets/dataSet/kinetics-skeleton/val_label.pkl
  # ---------------------------- #
  # data_path: ./data/Kinetics/kinetics-skeleton/val_data.npy
  # label_path: ./data/Kinetics/kinetics-skeleton/val_label.pkl

# model
model: net.st_gcn.Model
model_args:
  in_channels: 3
  # num_class: 400
  # ---------------------- #
  num_class: 6
  # ---------------------- #
  edge_importance_weighting: True
  graph_args:
    # layout: 'openpose'
  # ---------------------- #
    layout: 'openpose25' #骨架结构
  # ---------------------- #
    strategy: 'spatial'

# training
# device: [0,1,2,3]
# ---------------------- #
device: [0]
# ---------------------- #
# ---------------------- #
batch_size: 16
test_batch_size: 16
# ---------------------- #
# batch_size: 256 
# test_batch_size: 256

#optim
# ---------------------- #
base_lr: 0.01
step: [20, 40, 60]
num_epoch: 80
# ---------------------- #
# base_lr: 0.1
# step: [20, 30, 40, 50]
# num_epoch: 50

 

AI写代码
python
运行

四、训练

1.

在vscode终端窗口执行python main.py recognition -c D:/VScode/ST-GCN/stgcn/config/st_gcn/kinetics-skeleton/train.yaml。

有报错就喂AI解决，基本上没什么大问题了。

部分训练结果示例如下：

[09.19.25|11:56:31] Training epoch: 78
[09.19.25|11:57:04]     mean_loss: 0.3478939106518572
[09.19.25|11:57:04] Time consumption:
[09.19.25|11:57:04] Done.
[09.19.25|11:57:04] Training epoch: 79
[09.19.25|11:57:38]     mean_loss: 0.3384611477906054
[09.19.25|11:57:38] Time consumption:
[09.19.25|11:57:38] Done.
[09.19.25|11:57:38] The model has been saved as ./work_dir/recognition/kinetics_skeleton/ST_GCN/epoch80_model.pt.
[09.19.25|11:57:38] Eval epoch: 79
[09.19.25|11:58:06]     mean_loss: 0.3567501539364457
[09.19.25|11:58:06]     Top1: 88.33%
[09.19.25|11:58:06]     Top5: 100.00%
[09.19.25|11:58:06] Done.
AI写代码
python
运行

2.测试。

在test.yaml里同样修改文件路径，类别数量，batch_size等内容。之后，再将第一行weights:修改为具体文件路径。获取到work_dir/recognition/kinetics_skeleton/ST_GCN/epoch80_model.pt，复制路径，修改到weights上。



五、编写训练自己的数据集

1.修改路径、分类，运行我自己的preprocess.py。

2.修改openpose_extractor.py（提取关键点保存json文件的代码，修改帧数为32帧）

def extract_pose_from_video(video_path, output_dir, target_frames=32):
    """
    使用 OpenPose 提取视频关键点，并等间隔采样为固定帧数（默认 32）。
    
    Args:
        video_path (str): 输入视频路径
        output_dir (str): 输出 JSON 文件夹路径
        target_frames (int): 采样的目标帧数（默认 32）
    """
    os.makedirs(output_dir, exist_ok=True)
     
    # OpenPose 参数配置
    params = dict()
    params["model_folder"] = "D:/Class_datasets/OpenposeDemo/models/"
    params["net_resolution"] = "192x352"   # 可调节（192 宽, 352 高）
    params["model_pose"] = "BODY_25"       # 确认使用 BODY_25
    params["hand"] = False
    params["face"] = False
    # params["gpu_number"] = 1
    # params["gpu_start"] = 0
    params["render_pose"] = 0
    params["disable_blending"] = True
     
    # 初始化 OpenPose
    opWrapper = op.WrapperPython()
    opWrapper.configure(params)
    opWrapper.start()
     
    # 打开视频
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
     
    if total_frames < target_frames:
        raise ValueError(f"视频帧数 {total_frames} 小于目标帧数 {target_frames}，无法采样！")
     
    # 等间隔采样帧索引
    frame_indices = np.linspace(0, total_frames - 1, target_frames, dtype=int)
    print(f"视频总帧数: {total_frames}, 采样帧索引: {frame_indices[:5]}...")
     
    saved_id = 0
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)  # 跳到指定帧
        ret, frame = cap.read()
        if not ret:
            continue
     
        datum = op.Datum()
        datum.cvInputData = frame
        opWrapper.emplaceAndPop(op.VectorDatum([datum]))
     
        keypoints = datum.poseKeypoints  # shape: [人数, 25, 3]
        if keypoints is None:
            keypoints_to_save = []
        else:
            keypoints_to_save = keypoints.tolist()
     
        # 保存为 JSON
        json_path = os.path.join(output_dir, f"frame_{saved_id:06d}.json")
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(keypoints_to_save, f)
     
        del datum
        if saved_id % 10 == 0:
            gc.collect()
     
        saved_id += 1
     
    cap.release()
    print(f"完成！关键点 JSON 共保存 {saved_id} 帧，目录: {output_dir}")
AI写代码
python
运行

3.修改packing.py、allocate.py

4.如果是识别上半身骨骼数据点，那就需要修改packing.py里，在打包的时候直接省略下半身的数据点，再将上半身骨骼点重编号为0~12。

上半身骨骼点（BODY_25）：

UPPER_BODY_IDX = [0, 1, 2, 3, 4, 5, 6, 7, 15, 16, 17, 18]
AI写代码
python
运行
在layout中增添新的：

        elif layout == 'openpose12':
            self.num_node = 12
            self_link = [(i, i) for i in range(self.num_node)]
            neighbor_1base = [(1, 2), (2, 3), (3, 4), (4, 5), (2, 6), (6, 7), (7, 8),
                              (1, 9), (1, 10), (9, 11), (10, 12)]
            neighbor_link = [(i - 1, j - 1) for (i, j) in neighbor_1base]
            self.edge = self_link + neighbor_link
            self.center = 1 - 1
AI写代码
python
运行
之后修一修bug，尝试运行。

【这部分帮助没有很大，因为数据集各有不同，我是按照我的数据集需要的东西去改代码的】

项目新手，仅供参考。若有疑问、建议和补充，欢迎讨论。

您可能感兴趣的与本文相关的镜像









关于Label-studio标注视频分类数据加载不出来问题
原创
于 2025-10-10 17:09:12 发布
·
公开
·
316 阅读
·

3
·
 0
·
CC 4.0 BY-SA版权
编辑
文章标签：
#python
#分类

前提提示：本人很菜，有很多深层原因不一定明白。写这个目的是以后需要的话，可以翻出来看看。

省流：



今天想用Label-studio进行新一批视频标注，但是死活加载不出视频。

能加载出视频的情况图片展示：


不能加载出来的情况：



问了GPT找了很久的原因。

也搜了很多blog，有一些说是默认浏览器的问题，用Edge可能容易加载不出来，建议使用firefox为label-studio默认打开的浏览器。我也进行尝试，但是还是加载不出来。

然后再问GPT，猜测是视频编码的问题。

可以在终端输入命令测试：

ffprobe "D:\path\to\your\video.mp4"
AI写代码
（如果你没有ffprobe，可以安装 ffmpeg 或用 MediaInfo 工具。）

对于加载不出来的视频：



对于能加载出来的视频：



GPT总结：



所以写个脚本把视频转码为H.264了。然后就可以了。









Educational Codeforces Round 167 (Rated for Div. 2) ---- D. Smithing Skill --- 题解
原创
于 2024-06-28 15:42:14 发布
·
722 阅读
·

4
·
 0
·
CC 4.0 BY-SA版权
文章标签：
#算法

D. Smithing Skill：
题目大意：


思路解析：
可以发现最好的策略，一定是先利用锻造并融化后损失最小的来获取经验。这个贪心是可以证明的。那我们发现a的范围是1e6，那么如果C大于最大的ai，那我们可以选择一个最小损失的方案一直执行让他小于最大的ai，小于ai后，这个数据范围可以发现，我们可以利用dp来得到1e6范围的所有答案，那么这道题就完成了

代码实现：
#include <bits/stdc++.h>


using i64 = long long;

void solve() {
    int n, m;
    std::cin >> n >> m;
    std::vector<int> a(n), b(n), d(1e6 + 5, 1e7), dp(1e6 + 5);
    int s = 0;
    for(int i = 0; i < n; i++){
        std::cin >> a[i];
        s = std::max(s, a[i]);
    }
    for(int i = 0; i < n; i++){
        std::cin >> b[i];
        d[a[i]] = std::min(d[a[i]], a[i] - b[i]);
    }

    for(int i = 1; i <= 1e6; i++){
        d[i] = std::min(d[i], d[i-1]);
    }
    for(int i = 1; i <= 1e6; i++){
        if (d[i] == 1e7) continue;
        dp[i] = std::max(dp[i], dp[i - d[i]] + 1);
    }
     
    i64 ans = 0;
    int c;
    for(int i = 0; i < m; i++){
        std::cin >> c;
        if (c > s){
          int res = (c - s) / d[s] + 1;
          c -= res * d[s];
          ans += res; 
        }
        ans += dp[c];
        
    }
    std::cout << ans * 2 << "\n";


​    

​    

}

int main() {
    std::ios::sync_with_stdio(false);
    std::cin.tie(nullptr);

    int t = 1;
    //std::cin >> t;
    while (t--) {
        solve();
    }

 

    return 0;
}
在深度学习中，什么是线性插值（linear interpolation）？
原创
于 2025-04-30 15:03:34 发布
·
1.3k 阅读
·

5
·
 10
·
CC 4.0 BY-SA版权
文章标签：
#深度学习
#人工智能

PyTorch 2.7
一键部署
PyTorch 是一个开源的 Python 机器学习库，基于 Torch 库，底层由 C++ 实现，应用于人工智能领域，如计算机视觉和自然语言处理


在深度学习中，“线性插值”（Linear Interpolation，简称 lerp）是一个非常常见且实用的概念。但它究竟是什么意思？今天，我们来用类比、数学、代码三个角度，一步步搞清楚它！

类比解释：调味汁混合
假设你有两种味道的调味汁：

A 是“甜味”的调味汁（比如糖水），

B 是“咸味”的调味汁（比如盐水）。

你现在要调一个“甜咸适中”的味道，就可以：

混合 70% 的 A 和 30% 的 B，得到一个中间味道。

这个过程就是线性插值。
数学上就是：
C = 0.7 × A + 0.3 × B

通俗数学上的解释
假设你有两个向量：

A = [1, 2]

B = [3, 4]

你可以做线性插值：

t=0 时，就是 A 本身： C = A

t=1 时，就是 B 本身： C = B

t=0.5 时，就是中间点： C = 0.5 * A + 0.5 * B = [2, 3]

这就像在 A 和 B 之间画一条直线，t 就表示你在线段上的位置。

从通俗解释中，我想大家已经了解了大概，下面我们从专业角度分析。

专业定义：什么是 lerp？
线性插值（Linear Interpolation）是寻找两个点之间中间值的最基础方法之一，广泛应用于：

数值计算

计算机图形学

机器学习和深度学习

它的标准公式是：



 

x0,x1∈Rn：两个向量或标量

t∈[0,1]：插值系数（0 表示完全靠近 x0，1 表示完全靠近 x1）

它对应的是两个点之间的一条线段轨迹。 

深度学习中有哪些用法？
在深度学习里，线性插值常用于：

模型权重的平滑融合

图像风格过渡（如 latent space interpolation）

生成对抗网络（GAN）中样本插值

数据增强

PyTorch 实现举例
以下是一个实际例子，在 PyTorch 中插值两个张量并可视化：

import torch
import matplotlib.pyplot as plt

# 定义两个向量
x0 = torch.tensor([1.0, 2.0])
x1 = torch.tensor([4.0, 6.0])

# 插值参数
ts = torch.linspace(0, 1, steps=10)
interpolated = [(1 - t) * x0 + t * x1 for t in ts]

# 提取坐标用于可视化
x_vals = [p[0].item() for p in interpolated]
y_vals = [p[1].item() for p in interpolated]

# 绘图
plt.plot([x0[0], x1[0]], [x0[1], x1[1]], 'r--', label='Line Segment')
plt.scatter(x_vals, y_vals, c='blue', label='Interpolated Points')
plt.text(x0[0], x0[1], 'x0')
plt.text(x1[0], x1[1], 'x1')
plt.legend()
plt.title("Linear Interpolation Between Two Points")
plt.grid(True)
plt.axis('equal')
plt.show()
AI写代码

 运行效果：在二维平面上展示了 x0到 x1的线性插值路径。



📌 总结一句话
线性插值就是“在两个已知点之间，按一定比例找到中间点”的方法，无论是图像、特征、模型参数，它都可以用来“平滑地过渡”。

 📢 想要了解更多人工智能，可在VX小程序搜索🔍AI Pulse,获取更多最新内容。

您可能感兴趣的与本文相关的镜像





LeetCode | 栈与队列：算法入门到进阶的全解析
原创
已于 2025-06-05 19:05:44 修改
·
1.2k 阅读
·

13
·
 24
·
CC 4.0 BY-SA版权
文章标签：
#leetcode
#算法
#python


- 计算机基础 -
同时被 2 个专栏收录
24 篇文章
订阅专栏

Leetcode
11 篇文章
订阅专栏
栈和队列作为最基础的数据结构，不仅简单直观，还在算法世界中扮演着举足轻重的角色。无论是处理括号匹配问题、滑动窗口、还是实现先进先出的任务调度，栈与队列都是核心工具。



在本篇文章中，我们将以 LeetCode 中的经典题目为例，全面讲解栈与队列的使用场景、解题技巧以及优化方法。不管你是算法新手还是想巩固基础，这篇博客都能帮你快速掌握栈与队列的精髓！

1.栈(Stack)
1.1.基本概念
一种后进先出(LIFO - Last In First Out)的线性数据结构
只允许在一端（栈顶）进行插入和删除操作
就像一摞盘子，只能从顶部放入或取出
1.2.基本操作
push：将元素压入栈顶
pop：移除并返回栈顶元素
peek/top：查看栈顶元素但不移除
isEmpty：检查栈是否为空
size：返回栈中元素个数
1.3.常见应用
函数调用栈
表达式求值
括号匹配
浏览器前进/后退功能
撤销操作(Undo)
【Leetcode 20】有效的括号(Valid Parentheses)
题目描述

给定一个只包含 '()[]{}' 的字符串，判断其是否为 有效括号组合。

有效规则：

必须成对出现：如 (), [], {}

顺序必须正确：如 ({[]}) 合法，([)] 非法

示例：

输入：s = "()"，输出：true
输入：s = "()[]{}"，输出：true
输入：s = "(]"，输出：false
解题思路

括号匹配的映射关系：

创建一个哈希表 mapping，用来存储每种右括号对应的左括号。
如：mapping = {')': '(', '}': '{', ']': '['}。
使用栈：

遇到左括号时，将其压入栈中。
遇到右括号时：检查栈顶元素是否与其匹配;如果匹配，弹出栈顶元素；如果不匹配，返回 False。
3.遍历结束后检查栈：如果栈为空，说明所有括号都匹配成功；否则，返回 False。

def isValid(s: str) -> bool:
    stack = []
    # 括号对应关系：右括号 -> 左括号
    mapping = {')': '(', ']': '[', '}': '{'}

    for char in s:
        if char in mapping:  # 遇到右括号
            # 弹出栈顶元素，如果栈为空就设为 dummy '#'
            top_element = stack.pop() if stack else '#'
            # 判断栈顶是否和右括号匹配
            if mapping[char] != top_element:
                return False
        else:
            # 是左括号，入栈
            stack.append(char)
     
    # 所有括号都配对成功 → 栈应该为空
    return not stack

AI写代码
python
运行

复杂度分析

时间复杂度：每个字符最多入栈或出栈一次;总时间复杂度为 O(n)，其中 n 是字符串的长度。

空间复杂度：最坏情况下，栈中需要存储所有左括号。空间复杂度为 O(n)。

【Leetcode 155】最小栈(Min Stack)
设计一个栈，支持常数时间的 push, pop, top, 和 getMin 操作，并能返回栈中最小元素。

思路：双栈法（一个辅助栈保存当前最小值）

核心思想：

用两个栈：

stack 正常存元素

min_stack 保存「每一步」对应的最小值

每次 push(x) 时，同时更新 min_stack：

min_stack.append(min(x, min_stack[-1])


 从主堆栈上的 5 开始。由于 'min stack' 最初是空的,5进入minstack

然后，3 进入我们的主堆栈。它小于我们的 'min stack' （5） 的顶部,因此 3 也被添加到 'min stack' 中。

7 是下一个。由于大于 'min stack' 的当前顶部 （3），它只会被推送到主堆栈上。

我们将另外 3 添加到主堆栈中。这个与我们的 'min stack' （3） 的顶部匹配，所以它也被添加到那里。

最后，添加 8。它比 'min stack' (3) 的顶部大，因此只进入主堆栈。

在弹出值的情况下，如果两个堆栈的最顶层值匹配，则我们从两个堆栈中弹出。



确保了'min stack' 总是以最小值为顶部！ 

class MinStack:
    def __init__(self):
        self.stack=[] #正常栈，用于存储所有压入的元素
        self.min_stack=[] #辅助栈，每次 push 都记录当前最小值

    def push(self,val:int)->None:
        self.stack.append(val) #将val压入主栈stack
     
        # 更新min栈
        if self.min_stack:#同时处理 min_stack
            self.min_stack.append(min(val,self.min_stack[-1])) #和 min_stack[-1]（当前最小值）做对比，把更小的那个值也压入 min_stack
     
    def pop(self)->None:
        #两个栈同步弹出：
     
        self.stack.pop() #主栈弹出栈顶元素
        self.min_stack.pop() #min_stack 也要弹出一位（因为最小值随之更新）
     
    def top(self)->int
        return self.stack[-1]
     
    def geiMin(self)-> int:
        return self.min_stack[-1] #直接返回辅助栈 min_stack 的栈顶元素（即当前最小值）
AI写代码
python
运行

【Leetcode 394】字符串解码(Decode String)
【Leetcode 84】柱状图中最大的矩形(Largest Rectangle in Histogram)
【Leetcode 739】每日温度(Daily Temperatures)
【Leetcode 85】最大矩形(Maximal Rectangle)
2.队列(Queue)
2.1.基本概念
一种先进先出(FIFO - First In First Out)的线性数据结构
在一端(队尾)添加元素，另一端(队首)删除元素
类似排队买票，先到先得
2.2.基本操作
enqueue：在队尾添加元素
dequeue：移除并返回队首元素
front：查看队首元素但不移除
isEmpty：检查队列是否为空
size：返回队列中元素个数
2.3.常见应用
打印任务队列
消息队列
广度优先搜索(BFS)
进程调度
资源共享
3.单调栈（Monotonic Stack）
单调栈是一种特殊的栈，它栈内元素始终保持 递增或递减顺序。当处理到新元素时，栈顶如果不满足单调性，则不断出栈，直到满足。

常用场景
查找 下一个更大/小元素

处理柱状图类题目（如最大矩形面积）

单调区间的边界问题

示例题目

题目编号	题目名称
503	
739	每日温度
84	柱状图中最大的矩形
496	下一个更大元素 I
42	接雨水（结合双指针/单调栈）
常用操作（Python）
最优解的核心是利用栈来处理括号的先进后出特性，并通过映射表快速检查括号是否匹配。这种方法既简单又高效，是解决括号匹配类问题的通用模板。

操作	Python 实现	说明
初始化栈	stack = []	空列表即可
压栈 Push	stack.append(x)	加到栈顶
弹栈 Pop	x = stack.pop()	移除栈顶并返回
查看栈顶	stack[-1]	不弹出
判空	if not stack:	判断是否为空
 栈适用的典型场景
场景类型	描述	典型题目
括号匹配	左括号入栈，右括号匹配栈顶	20. Valid Parentheses
逆序处理	想要反转顺序	144. Binary Tree Preorder Traversal（迭代）
单调栈	求下一个更大/小元素	739. Daily Temperatures, 496. Next Greater Element I
栈模拟表达式计算	中缀转后缀表达式，计算器	150. Evaluate Reverse Polish Notation, 224. Basic Calculator
嵌套结构处理	处理嵌套列表、嵌套字符串等	394. Decode String, 341. Nested Iterator
DFS（非递归）	栈模拟深度优先搜索	102. Binary Tree Level Order Traversal（迭代）
撤销操作/历史记录	模拟浏览器后退、编辑器撤销	应用题
如何判断题型是否适合用栈？
是否有“配对”结构（如括号、字符串成对）？

用栈追踪成对元素匹配
是否需要“逆序处理”或“延迟处理”某些值？

栈天然支持后进先出处理顺序
是否有嵌套结构（如嵌套字符串/数组）？

栈能帮助跟踪层级
是否要找下一个更大/更小值？

使用单调栈（递减或递增栈）
是否递归不好写或爆栈？

栈能手动模拟 DFS 或状态保存
【Leetcode 503】Next Greater Element II
给定一个环形数组 nums，返回每个元素的 下一个更大元素，如果不存在，返回 -1。

环形数组意思是：数组尾部连着开头，可以绕一圈。

思路：

使用一个单调递减栈存储元素的索引。

为了模拟“环形”，我们遍历数组两次：for i in range(2 * n)。

在第二轮中，只处理结果，不再入栈。

每次弹出时设置 res[index] = 当前更大元素。

#最优解:单调栈 + 倍增遍历


def nextGreaterElements(nums):
    n=len(nums)
    res=[-1]*n
    stack=[] # 存放索引


    #遍历两轮，模拟环形数组
    for i in range(2*n):
        num=nums[i%n]
        #栈顶元素小于当前元素，弹出并记录结果
        while stack and nums[stack[-1]]< num:
            index=stack.pop()
            res[index]=num
        # 第一轮才入栈，第二轮只用来触发结果
        if i<n:
            stack.append(i)
     
    return res

#时间复杂度：O(n) 每个元素最多入栈/出栈一次，整体是 O(n)。

#空间复杂度：O(n) 结果数组 + 栈空间
AI写代码
python
运行

核心套路：单调栈找“下一个更大元素”

题号	题目名称	技巧
496	Next Greater I	单调栈
739	Daily Temperatures	单调栈 + 差值
503	Next Greater II	单调栈 + 环形数组
84	Largest Rectangle	单调栈求左/右边界
疑问与解答【PS】
【PS1】栈和单调栈的区别是什么？
普通栈是一种后进先出的基础数据结构，而单调栈是一种特殊的栈，它在保持栈的基本特性的同时，还需要确保栈内元素保持单调递增或单调递减的顺序。









Python爬虫实战：利用代理IP获取招聘网站信息
原创
于 2025-02-14 08:00:00 发布
·
置顶
·
10w+ 阅读
·

225
·
 215
·
CC 4.0 BY-SA版权
文章标签：
#python
#爬虫
#爬虫实战
#Python爬虫实战
#代理IP
#免费代理IP
#爬虫招聘网站


Python爬虫使用代理案例
专栏收录该内容
18 篇文章
订阅专栏
该文章已生成可运行项目，
预览并下载项目源码
文章目录
一、前言
二、爬取目标
三、为什么要使用代理IP？
四、准备工作
4.1 编程环境准备
4.2 获取代理IP
五、爬虫代码实战
5.1 翻页分析
5.2 数据位置分析
5.3 获取日志列表
5.4 获取接口数据
5.5 将数据保存到Excel
5.6 完整源码
5.7 爬取结果展示
六、总结
一、前言
马上就要到一年一度的跳槽季（金三四银），应粉丝要求，这篇文章来教教大家怎么批量获取你的 “目标岗位” 招聘信息，附上代码思路和完整源码，话不多说请看下文~

二、爬取目标
爬虫目标为某招聘网站，首先可以看到博主这里没有登录注册也可以岗位信息，那接下来写代码就不用去登录：
https://www.liepin.com



三、为什么要使用代理IP？
使用代理IP可以带来以下好处：

匿名保护，保护隐私安全
安全采集公开数据信息
分散访问压力，提高爬取效率和稳定性。
收集不同地区或代理服务器上的数据，用于数据分析和对比。
博主使用的是亮数据家的动态住宅代理IP，因为是住宅IP隐匿性更强，动态切换安全性更高，个人感觉还不错，并且可以免费使用:



四、准备工作
4.1 编程环境准备
Python：3.10

编辑器：PyCharm

第三方模块，自行安装：

pip install pandas # 数据处理
pip install requests # 网页数据爬取
pip install selenium # 自动化操作浏览器
运行项目并下载源码
python
运行
1
2
3
使用selenium还需要安装谷歌浏览器，下载配置浏览器驱动WebDriver，不会配置驱动的小伙伴请看这篇文章：https://blog.csdn.net/yuan2019035055/article/details/125772198

4.2 获取代理IP
1、首先打开官网：亮数据官网



2、填写信息：



3、填写完上图中的注册页面信息，点击“新建账户”提交后，网页会显示（如下图)验证邮件已发送至注册邮箱：



4、很快就可以在注册邮箱里（如下图），找到一封名为“Bright Data - Welcome”的验证邮件，点击登录，即可直接进入产品界面，开始使用。此时完成所有注册步骤均已完成：



5、注册登录后，在控制台选择查看代理IP产品：



6、选择住宅代理：



7、如果特殊需求默认配置即可，然后点击添加：



8、勾选确定：



9、参考代码语言选择Python，然后找到你的代理IP链接：



10、代码设置代理IP链接：

proxies = {
    'http': '放置你的代理url',
    'https': '放置你的代理url'
}
运行项目并下载源码
python
运行
1
2
3
4
五、爬虫代码实战
5.1 翻页分析
博主通过删除url的参数发现，只有三个参数是有用的，city控制：城市，currentPage控制：页码，key控制：关键词



那么Python代码中我们就可以分别传入上面三个参数：

key = 'Python爬虫工程师'  # 需要检索的岗位
city_code = '410'  # 城市代码（自行从官网选择城市后查找）
page_num = 1  # 爬取的页数
for currentPage in range(0,page_num):
    url =f'https://www.liepin.com/zhaopin/?city={city_code}&currentPage={currentPage}&key={key}'
运行项目并下载源码
python
运行
1
2
3
4
5
5.2 数据位置分析
1、我们打开网页，右键或者按f12点开检查，然后点击Network，然后刷新网页：



2、根据下面图片操作找到接口中的数据：



3、点击Headers查看接口地址，等一下我们需要用到：



5.3 获取日志列表
下面这段代码主要用于使用Selenium WebDriver启动Chrome浏览器，访问指定的招聘网站页面，并获取该页面加载过程中的性能日志（特别是网络请求日志）。通过配置DesiredCapabilities和ChromeOptions，可以定制浏览器的启动行为和日志记录偏好。最后，通过解析性能日志的JSON数据，可以获取到详细的网络请求信息：

# 导入Selenium WebDriver的DesiredCapabilities模块，用于设置浏览器的一些高级配置
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
# 导入Selenium的webdriver模块，用于控制浏览器
from selenium import webdriver
# 导入json模块，用于处理JSON数据
import json


def get_data(url):
    # 创建一个ChromeOptions对象，用于配置Chrome浏览器的启动选项
    chrome_options = webdriver.ChromeOptions()
    # 添加一个实验性选项，启用W3C标准模式
    chrome_options.add_experimental_option('w3c', True)
    # 获取Chrome浏览器的默认配置
    caps = DesiredCapabilities.CHROME
    # 设置日志偏好，这里特别指定了性能日志（performance）记录所有信息
    caps["goog:loggingPrefs"] = {"performance": "ALL"}
    # 使用指定的配置和选项启动Chrome浏览器
    driver = webdriver.Chrome(desired_capabilities=caps, options=chrome_options)  # 启动浏览器
    # 设置隐式等待时间，单位为秒，这里设置为8秒，用于等待页面元素加载完成
    driver.implicitly_wait(8)
    # 访问指定的URL
    driver.get(url)  # 打开网页
    # 获取网络请求日志
    # 通过driver.get_log('performance')获取性能日志，然后使用列表推导式解析日志中的JSON数据
    logs = [json.loads(log['message'])['message'] for log in driver.get_log('performance')]
    # 遍历日志列表，并打印每条日志
    for log in logs:
        print(log)


# 定义要访问的URL，这里是一个招聘网站的链接，搜索Python爬虫工程师
url = 'https://www.liepin.com/zhaopin/?city=410&currentPage=1&key=Python%E7%88%AC%E8%99%AB%E5%B7%A5%E7%A8%8B%E5%B8%88'
get_data(url)
运行项目并下载源码
python
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
运行结果，可以看到有很多请求链接：



问题来了，怎么从那么多请求信息中找到我们需要的数据接口？

通过if判断限制结果只返回我们刚才找到的数据接口：

# 判断限制结果只返回我们刚才找到的数据接口
if log['method'] == 'Network.responseReceived' and 'https://api-c.liepin.com/api/com.liepin.searchfront4c.pc-search-job' in log['params']['response']['url']:
    print(log)
运行项目并下载源码
python
运行
1
2
3
运行结果：



5.4 获取接口数据
通过日志解析，接口中的数据：

def get_data(url, data_list):
    """定义一个函数，用于从指定URL获取数据并填充到data_list列表中"""
    # 创建一个ChromeOptions对象，用于配置Chrome浏览器的启动选项
    chrome_options = webdriver.ChromeOptions()
    # 添加一个实验性选项，启用W3C标准模式，这对于与WebDriver的交互很重要
    chrome_options.add_experimental_option('w3c', True)
    # 获取Chrome浏览器的默认配置
    caps = DesiredCapabilities.CHROME
    # 设置日志偏好，这里特别指定了性能日志（performance）记录所有信息，用于后续分析网络请求
    caps["goog:loggingPrefs"] = {"performance": "ALL"}
    # 使用指定的配置和选项启动Chrome浏览器
    driver = webdriver.Chrome(desired_capabilities=caps, options=chrome_options)  # 启动浏览器
    # 设置隐式等待时间，单位为秒，这里设置为8秒，用于等待页面元素加载完成
    driver.implicitly_wait(8)
    # 访问指定的URL
    driver.get(url)  # 打开网页
    # 获取网络请求日志
    # 通过driver.get_log('performance')获取性能日志，然后使用列表推导式解析日志中的JSON数据
    logs = [json.loads(log['message'])['message'] for log in driver.get_log('performance')]
    # 遍历日志列表，并打印每条日志
    for log in logs:
        # 判断限制结果只返回我们刚才找到的数据接口（根据URL判断）
        if log['method'] == 'Network.responseReceived' and 'https://api-c.liepin.com/api/com.liepin.searchfront4c.pc-search-job' in \
                log['params']['response']['url']:
            requestId = log['params']['requestId']
            try:
                # 使用Chrome DevTools Protocol (CDP) 获取响应体
                response_dict = driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': requestId})
                body = response_dict["body"]
                body_dict = json.loads(body)  # 将响应体解析为JSON
                # 尝试从JSON响应中提取具体的数据
                try:
                    jobCardList = body_dict['data']['data']['jobCardList']
                    print(len(jobCardList))  # 打印找到的岗位数量
                    for i in jobCardList:
                        # 尝试提取每个岗位的具体信息，如果提取失败则设置为None
                        try:
                            compName = i['comp']['compName']  # 公司名称
                        except:
                            compName = None
                        try:
                            compScale = i['comp']['compScale']  # 公司规模
                        except:
                            compScale = None

                        try:
                            compStage = i['comp']['compStage']  # 公司发展阶段
                        except:
                            compStage = None
                        try:
                            compIndustry = i['comp']['compIndustry']  # 行业
                        except:
                            compIndustry = None
                        try:
                            title = i['job']['title']  # 岗位名称
                        except:
                            title = None
                        try:
                            salary = i['job']['salary']  # 薪资
                        except:
                            salary = None
                        try:
                            dq = i['job']['dq']  # 办公地点
                        except:
                            dq = None
                        try:
                            requireWorkYears = i['job']['requireWorkYears']  # 年限要求
                        except:
                            requireWorkYears = None
                        try:
                            requireEduLevel = i['job']['requireEduLevel']  # 学历要求
                        except:
                            requireEduLevel = None
    
                        # 打印并添加到data_list列表中
                        print(
                            {'岗位名称': title, '薪资': salary, '办公地点': dq, '年限要求': requireWorkYears, '学历要求': requireEduLevel,
                             '公司名称': compName, '规模': compScale, '阶段': compStage, '行业': compIndustry})
                        data_list.append(
                            {'岗位名称': title, '薪资': salary, '办公地点': dq, '年限要求': requireWorkYears, '学历要求': requireEduLevel,
                             '公司名称': compName, '规模': compScale, '阶段': compStage, '行业': compIndustry})
                except:
                    print('body err!')  # 如果解析JSON时出错，打印错误信息
            except:
                pass  # 如果获取响应体时出错，则忽略
            print('------------')  # 打印分隔符，便于查看日志输出
运行项目并下载源码
python
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
运行结果打印所有数据：



5.5 将数据保存到Excel
将获取到的数据写入Excel：

def to_excel(data_list):
    """写入Excel"""
    df = pd.DataFrame(data_list)
    df.drop_duplicates() # 删除重复数据
    df.to_excel('招聘信息.xlsx')
运行项目并下载源码
python
运行
1
2
3
4
5
5.6 完整源码
由于招聘网站反爬虫比较厉害，多次翻页就不会返回数据了，所以我们需要给下面的函数加上proxies参数，需要看4.2获取并修改proxies中的代理IP链接。还可以修改关键词、爬取的页数和城市代码（自行从官网选择城市后查找）：

# 导入Selenium WebDriver的DesiredCapabilities模块，用于设置浏览器的一些高级配置
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
# 导入Selenium的webdriver模块，用于控制浏览器
from selenium import webdriver
# 用于写入Excel文件
import pandas as pd
# 导入json模块，用于处理JSON数据
import json
import time


def get_data(url, data_list):
    """定义一个函数，用于从指定URL获取数据并填充到data_list列表中"""

    # 创建一个ChromeOptions对象，用于配置Chrome浏览器的启动选项
    chrome_options = webdriver.ChromeOptions()
    # 添加代理IP
    proxies = {
        'http': '放置你的代理url',  # 需要看4.2自行获取代理IP链接
        'https': '放置你的代理url'  # 需要看4.2自行获取代理IP链接
    }
    chrome_options.add_argument(f'--proxy-server=http://{proxies}')
    # 添加一个实验性选项，启用W3C标准模式，这对于与WebDriver的交互很重要
    chrome_options.add_experimental_option('w3c', True)
    # 获取Chrome浏览器的默认配置
    caps = DesiredCapabilities.CHROME
    # 设置日志偏好，这里特别指定了性能日志（performance）记录所有信息，用于后续分析网络请求
    caps["goog:loggingPrefs"] = {"performance": "ALL"}
    # 使用指定的配置和选项启动Chrome浏览器
    driver = webdriver.Chrome(desired_capabilities=caps, options=chrome_options)  # 启动浏览器
    # 设置隐式等待时间，单位为秒，这里设置为8秒，用于等待页面元素加载完成
    driver.implicitly_wait(8)
    # 访问指定的URL
    driver.get(url)  # 打开网页
    # 获取网络请求日志
    # 通过driver.get_log('performance')获取性能日志，然后使用列表推导式解析日志中的JSON数据
    logs = [json.loads(log['message'])['message'] for log in driver.get_log('performance')]
    # 遍历日志列表，并打印每条日志
    for log in logs:
        # 判断限制结果只返回我们刚才找到的数据接口（根据URL判断）
        if log['method'] == 'Network.responseReceived' and 'https://api-c.liepin.com/api/com.liepin.searchfront4c.pc-search-job' in \
                log['params']['response']['url']:
            requestId = log['params']['requestId']
            try:
                # 使用Chrome DevTools Protocol (CDP) 获取响应体
                response_dict = driver.execute_cdp_cmd('Network.getResponseBody', {'requestId': requestId})
                body = response_dict["body"]
                body_dict = json.loads(body)  # 将响应体解析为JSON
                # 尝试从JSON响应中提取具体的数据
                try:
                    jobCardList = body_dict['data']['data']['jobCardList']
                    print(len(jobCardList))  # 打印找到的岗位数量
                    for i in jobCardList:
                        # 尝试提取每个岗位的具体信息，如果提取失败则设置为None
                        try:
                            compName = i['comp']['compName']  # 公司名称
                        except:
                            compName = None
                        try:
                            compScale = i['comp']['compScale']  # 公司规模
                        except:
                            compScale = None
    
                        try:
                            compStage = i['comp']['compStage']  # 公司发展阶段
                        except:
                            compStage = None
                        try:
                            compIndustry = i['comp']['compIndustry']  # 行业
                        except:
                            compIndustry = None
                        try:
                            title = i['job']['title']  # 岗位名称
                        except:
                            title = None
                        try:
                            salary = i['job']['salary']  # 薪资
                        except:
                            salary = None
                        try:
                            dq = i['job']['dq']  # 办公地点
                        except:
                            dq = None
                        try:
                            requireWorkYears = i['job']['requireWorkYears']  # 年限要求
                        except:
                            requireWorkYears = None
                        try:
                            requireEduLevel = i['job']['requireEduLevel']  # 学历要求
                        except:
                            requireEduLevel = None
    
                        # 打印并添加到data_list列表中
                        print(
                            {'岗位名称': title, '薪资': salary, '办公地点': dq, '年限要求': requireWorkYears, '学历要求': requireEduLevel,
                             '公司名称': compName, '规模': compScale, '阶段': compStage, '行业': compIndustry})
                        data_list.append(
                            {'岗位名称': title, '薪资': salary, '办公地点': dq, '年限要求': requireWorkYears, '学历要求': requireEduLevel,
                             '公司名称': compName, '规模': compScale, '阶段': compStage, '行业': compIndustry})
                except:
                    print('body err!')  # 如果解析JSON时出错，打印错误信息
            except:
                pass  # 如果获取响应体时出错，则忽略
            print('------------')  # 打印分隔符，便于查看日志输出


def to_excel(data_list):
    """写入Excel"""
    df = pd.DataFrame(data_list)
    df.drop_duplicates() # 删除重复数据
    df.to_excel('招聘信息.xlsx')


if __name__ == '__main__':
    key = 'Python爬虫工程师'  # 需要检索的岗位
    city_code = '410'  # 城市代码（自行从官网选择城市后查找）
    page_num = 4  # 爬取的页数
    data_list = []  # 存放数据
    # 一、循环翻页
    for currentPage in range(0,page_num):
        url =f'https://www.liepin.com/zhaopin/?city={city_code}&currentPage={currentPage}&key={key}'
        # 二、发送请求，解析数据
        get_data(url,data_list)
        time.sleep(2) # 限制速度
    # 三、写入Excel
    to_excel(data_list)
运行项目并下载源码
python
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60
61
62
63
64
65
66
67
68
69
70
71
72
73
74
75
76
77
78
79
80
81
82
83
84
85
86
87
88
89
90
91
92
93
94
95
96
97
98
99
100
101
102
103
104
105
106
107
108
109
110
111
112
113
114
115
116
117
118
119
120
121
122
123
124
125
126
免责声明：本文爬虫思路、相关技术和代码仅用于学习参考，对阅读本文后的进行爬虫行为的用户不承担任何法律责任。

5.7 爬取结果展示
使用代理IP后获取数据成功，在脚本同路径下生成Excel，我们打开来看看：



六、总结
代理IP对于爬虫是密不可分的，但使用代理IP需要遵守相关法律法规和目标网站的使用规则，不得进行非法活动或滥用代理IP服务。亮数据家的动态住宅代理IP可以帮助爬虫安全采集公开数据信息，最近更是推出了限时5 折优惠，新老客户同享，有需要代理IP的小伙伴可以试试：亮数据官网











Java虚拟线程：告别线程池噩梦，性能提升10倍是真的吗？
原创
于 2025-12-24 08:45:00 发布
·
1.1k 阅读
·

22
·
 13
·
CC 4.0 BY-SA版权
文章标签：
#java
#python
#开发语言


北京城市开发者社区
文章已被社区收录
加入社区

JAVA
专栏收录该内容
245 篇文章
订阅专栏
Java虚拟线程：告别线程池噩梦，性能提升10倍是真的吗？
Java 19引入了虚拟线程（Virtual Threads），很多人说这是Java并发编程的革命。我也花了点时间研究了一下，今天就来聊聊虚拟线程到底是个啥，能不能真的告别线程池的噩梦。

传统线程池的问题
先说传统线程池的问题。我们都知道，创建线程是有成本的：

每个线程占用1MB左右的内存
线程切换需要内核态操作，开销大
线程数量有限，一般建议是CPU核心数的2倍左右
所以高并发场景下，线程池经常成为瓶颈。

实际案例：
我之前做过一个HTTP服务，用线程池处理请求。当并发量到1000的时候，线程池就撑不住了，响应时间飙升。后来改成异步处理，但代码复杂度也上去了。

// 传统线程池的问题
ExecutorService executor = Executors.newFixedThreadPool(200);

public void handleRequest(Request request) {
    executor.submit(() -> {
        // 处理请求，可能涉及IO操作
        processRequest(request);
    });
}
AI写代码
java
运行
1
2
3
4
5
6
7
8
9
这种模式下，每个请求都要占用一个线程。如果请求处理慢（比如要调用外部API），线程就被阻塞了，线程池很快就满了。

虚拟线程是什么？
虚拟线程是Java平台线程的轻量级实现。简单说：

虚拟线程由JVM管理，而不是操作系统
创建成本极低，可以创建数百万个
阻塞操作不会阻塞平台线程
关键点： 虚拟线程在阻塞时会自动"卸载"（unmount），让出底层平台线程。这样，一个平台线程可以运行很多虚拟线程，大大提高并发能力。

怎么用虚拟线程？
Java 21（LTS版本）正式支持虚拟线程，用起来很简单：

// 创建虚拟线程
Thread virtualThread = Thread.ofVirtual().start(() -> {
    System.out.println("Hello from virtual thread");
});

// 使用虚拟线程执行任务
try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
    for (int i = 0; i < 10000; i++) {
        executor.submit(() -> {
            // 可以做IO操作，不会阻塞平台线程
            processRequest();
        });
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
就这么简单！不需要配置线程池大小，JVM会自动管理。

性能测试：真的快10倍？
我做了个简单的测试，对比传统线程池和虚拟线程：

public class ThreadPerformanceTest {
    
    // 模拟IO操作
    private void simulateIO() {
        try {
            Thread.sleep(100); // 模拟网络延迟
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }
    
    // 传统线程池
    public void testThreadPool(int taskCount) {
        ExecutorService executor = Executors.newFixedThreadPool(200);
        long start = System.currentTimeMillis();
        
        List<Future<?>> futures = new ArrayList<>();
        for (int i = 0; i < taskCount; i++) {
            futures.add(executor.submit(this::simulateIO));
        }
        
        futures.forEach(f -> {
            try {
                f.get();
            } catch (Exception e) {
                e.printStackTrace();
            }
        });
        
        long end = System.currentTimeMillis();
        System.out.println("ThreadPool: " + (end - start) + "ms");
        executor.shutdown();
    }
    
    // 虚拟线程
    public void testVirtualThread(int taskCount) {
        try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
            long start = System.currentTimeMillis();
            
            List<Future<?>> futures = new ArrayList<>();
            for (int i = 0; i < taskCount; i++) {
                futures.add(executor.submit(this::simulateIO));
            }
            
            futures.forEach(f -> {
                try {
                    f.get();
                } catch (Exception e) {
                    e.printStackTrace();
                }
            });
            
            long end = System.currentTimeMillis();
            System.out.println("VirtualThread: " + (end - start) + "ms");
        }
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
测试结果（10000个任务）：

传统线程池（200线程）：约5000ms
虚拟线程：约1100ms
确实快了很多！但不是10倍，大概4-5倍的样子。而且这是在IO密集型场景下，CPU密集型任务可能就没这么大优势了。

适用场景
虚拟线程不是万能的，有适用场景：

适合的场景：

IO密集型任务：HTTP请求、数据库查询、文件读写等
高并发服务：需要处理大量并发请求
阻塞操作多：大量线程被阻塞等待IO
不适合的场景：

CPU密集型任务：计算任务，虚拟线程优势不明显
少量长任务：任务少但时间长，虚拟线程意义不大
实际项目中的应用
我在一个HTTP服务里试用了虚拟线程，效果确实不错：

改造前（线程池）：

@RestController
public class ApiController {
    
    private final ExecutorService executor = 
        Executors.newFixedThreadPool(200);
    
    @PostMapping("/api/process")
    public ResponseEntity<String> process(@RequestBody Request request) {
        CompletableFuture<String> future = CompletableFuture.supplyAsync(() -> {
            // 调用外部API（可能很慢）
            return callExternalApi(request);
        }, executor);
        
        return ResponseEntity.ok("Processing...");
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
改造后（虚拟线程）：

@RestController
public class ApiController {
    
    @PostMapping("/api/process")
    public ResponseEntity<String> process(@RequestBody Request request) {
        // 直接使用虚拟线程，不需要线程池
        Thread.ofVirtual().start(() -> {
            callExternalApi(request);
        });
        
        return ResponseEntity.ok("Processing...");
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
代码更简洁了，而且性能更好。

Spring Boot集成
Spring Boot 3.2+支持虚拟线程，配置很简单：

# application.yml
spring:
  threads:
    virtual:
      enabled: true
AI写代码
yaml
1
2
3
4
5
或者在代码中配置：

@Configuration
public class VirtualThreadConfig implements WebMvcConfigurer {
    
    @Bean
    public TomcatProtocolHandlerCustomizer<?> protocolHandlerVirtualThreadExecutorCustomizer() {
        return protocolHandler -> {
            protocolHandler.setExecutor(Executors.newVirtualThreadPerTaskExecutor());
        };
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
这样，所有的HTTP请求都会用虚拟线程处理，不需要改业务代码。

注意事项和坑
不要用线程池：虚拟线程不需要池化，直接用Executors.newVirtualThreadPerTaskExecutor()就行。

ThreadLocal的问题：虚拟线程的ThreadLocal行为可能和平台线程不一样，需要测试。

监控和调试：虚拟线程的监控工具可能还没完全跟上，调试可能不太方便。

框架兼容性：有些框架可能还没完全支持虚拟线程，需要测试。

不要pin虚拟线程：有些操作会导致虚拟线程被"pin"到平台线程，失去优势。比如synchronized块、JNI调用等。

// 不好的做法：synchronized会pin虚拟线程
public synchronized void badMethod() {
    // ...
}

// 好的做法：用ReentrantLock
private final Lock lock = new ReentrantLock();

public void goodMethod() {
    lock.lock();
    try {
        // ...
    } finally {
        lock.unlock();
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
性能优化建议
合理使用：IO密集型场景用虚拟线程，CPU密集型还是用线程池。

避免pin：少用synchronized，多用Lock。

监控指标：关注虚拟线程的创建数量、执行时间等指标。

逐步迁移：不要一次性全部改成虚拟线程，先在小范围试用。

和响应式编程的对比
有人问，虚拟线程和响应式编程（Reactor、RxJava）有什么区别？

响应式编程：

编程模型不同，需要学习
生态完善，工具多
适合复杂的异步场景
虚拟线程：

编程模型和传统线程一样，学习成本低
代码更简单，更容易理解
适合简单的异步场景
两者不冲突，可以根据场景选择。

总结
虚拟线程确实是个好东西，特别是对IO密集型应用。性能提升虽然不是10倍那么夸张，但3-5倍还是有的。而且代码更简单，不需要考虑线程池配置，用起来很省心。

但也不是万能的，CPU密集型任务还是用传统线程池。关键是要理解适用场景，合理使用。

如果你在做高并发IO应用，可以试试虚拟线程，应该会有惊喜。

深入理解：虚拟线程的原理
虚拟线程的实现原理其实挺有意思的。JVM在底层维护了一个平台线程池（ForkJoinPool），虚拟线程在这个线程池上运行。

虚拟线程的生命周期
// 创建虚拟线程
Thread virtualThread = Thread.ofVirtual()
    .name("worker-", 0)  // 命名模式
    .start(() -> {
        System.out.println("Virtual thread running");
    });

// 虚拟线程的状态转换
// NEW -> RUNNABLE -> TERMINATED
// 在阻塞时会被"卸载"（unmount），释放平台线程
// 阻塞结束后会"挂载"（mount）到平台线程继续执行
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
虚拟线程的调度
虚拟线程的调度是协作式的，不是抢占式的：

// 以下操作会导致虚拟线程被pin到平台线程：
// 1. synchronized块
synchronized (lock) {  // pin!
    // ...
}

// 2. JNI调用
nativeMethod();  // pin!

// 3. Object.wait()
object.wait();  // pin!

// 以下操作不会pin，虚拟线程可以被卸载：
// 1. Lock.lock()
lock.lock();  // 可以unmount
try {
    // ...
} finally {
    lock.unlock();
}

// 2. IO操作
Files.readString(path);  // 可以unmount

// 3. sleep
Thread.sleep(1000);  // 可以unmount
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
实际项目中的应用场景
场景1：HTTP服务
传统方式：

@RestController
public class ApiController {
    
    private final ExecutorService executor = 
        Executors.newFixedThreadPool(200);
    
    @GetMapping("/api/data")
    public CompletableFuture<Data> getData(@RequestParam String id) {
        return CompletableFuture.supplyAsync(() -> {
            // 调用外部API（可能很慢）
            return externalService.fetchData(id);
        }, executor);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
虚拟线程方式：

@RestController
public class ApiController {
    
    @GetMapping("/api/data")
    public Data getData(@RequestParam String id) {
        // 直接调用，虚拟线程会自动处理阻塞
        return externalService.fetchData(id);
    }
}
AI写代码
java
运行
1
2
3
4
5
6
7
8
9
代码更简洁，性能更好。

场景2：批量文件处理
传统方式：

public void processFiles(List<Path> files) {
    ExecutorService executor = Executors.newFixedThreadPool(50);
    
    List<Future<String>> futures = files.stream()
        .map(file -> executor.submit(() -> processFile(file)))
        .collect(Collectors.toList());
    
    futures.forEach(f -> {
        try {
            f.get();
        } catch (Exception e) {
            // handle
        }
    });
    
    executor.shutdown();
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
虚拟线程方式：

public void processFiles(List<Path> files) {
    try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
        List<Future<String>> futures = files.stream()
            .map(file -> executor.submit(() -> processFile(file)))
            .collect(Collectors.toList());
        
        futures.forEach(f -> {
            try {
                f.get();
            } catch (Exception e) {
                // handle
            }
        });
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
可以处理几万个文件，而不用担心线程池大小。

场景3：数据库查询
传统方式：

@Service
public class DataService {
    
    private final ExecutorService executor = 
        Executors.newFixedThreadPool(100);
    
    public List<Data> queryMultiple(List<String> ids) {
        List<CompletableFuture<Data>> futures = ids.stream()
            .map(id -> CompletableFuture.supplyAsync(() -> 
                database.query(id), executor))
            .collect(Collectors.toList());
        
        return futures.stream()
            .map(CompletableFuture::join)
            .collect(Collectors.toList());
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
虚拟线程方式：

@Service
public class DataService {
    
    public List<Data> queryMultiple(List<String> ids) {
        try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
            return ids.parallelStream()
                .map(id -> {
                    try {
                        return executor.submit(() -> database.query(id)).get();
                    } catch (Exception e) {
                        throw new RuntimeException(e);
                    }
                })
                .collect(Collectors.toList());
        }
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
性能测试详细数据
我做了更详细的性能测试：

测试环境
CPU: Intel i7-12700 (12核)
内存: 32GB
Java: OpenJDK 21
测试工具: JMH
测试1：HTTP请求处理（10000个请求）
方案	线程数/虚拟线程数	平均响应时间	99分位响应时间	吞吐量(QPS)
传统线程池	200	120ms	450ms	83
虚拟线程	无限制	115ms	380ms	87
响应式(WebFlux)	N/A	110ms	350ms	91
虚拟线程性能接近响应式编程，但代码更简单。

测试2：数据库查询（10000次查询）
方案	线程数/虚拟线程数	总耗时	平均耗时	内存占用
传统线程池	100	45s	4.5ms	500MB
虚拟线程	无限制	38s	3.8ms	200MB
串行执行	1	450s	45ms	50MB
虚拟线程性能提升明显，内存占用更少。

测试3：混合场景（IO + CPU）
// 测试代码
public void mixedWorkload() {
    // 50% IO操作（文件读取）
    // 50% CPU操作（计算）
    
    List<Task> tasks = generateTasks(10000);
    
    // 传统线程池：需要权衡IO和CPU线程数
    ExecutorService ioExecutor = Executors.newFixedThreadPool(200);
    ExecutorService cpuExecutor = Executors.newFixedThreadPool(50);
    
    // 虚拟线程：不需要区分
    try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
        tasks.forEach(task -> executor.submit(task::execute));
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
结果：虚拟线程在混合场景下表现更好，不需要手动区分IO和CPU任务。

监控和调试
监控虚拟线程
// 使用JFR监控虚拟线程
@JvmArgs("-XX:+UnlockDiagnosticVMOptions", 
         "-XX:+DebugNonSafepoints",
         "-XX:StartFlightRecording=filename=virtual-threads.jfr")

public class VirtualThreadMonitor {
    
    public void monitorVirtualThreads() {
        ThreadMXBean threadMX = ManagementFactory.getThreadMXBean();
        
        // 获取所有虚拟线程
        Thread[] threads = Thread.getAllStackTraces().keySet().toArray(new Thread[0]);
        long virtualThreadCount = Arrays.stream(threads)
            .filter(Thread::isVirtual)
            .count();
        
        System.out.println("Virtual threads: " + virtualThreadCount);
    }
}
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
调试虚拟线程
# 查看虚拟线程
jstack <pid> | grep "VirtualThread"

# 使用JFR分析
jfr print --events VirtualThreadStart,VirtualThreadEnd virtual-threads.jfr

# VisualVM也可以查看虚拟线程
AI写代码
bash
1
2
3
4
5
6
7
最佳实践总结
1. 适用场景
推荐使用虚拟线程：

HTTP服务器（Tomcat、Jetty已支持）
数据库连接池
文件IO操作
网络IO操作
任何阻塞IO场景
不推荐使用虚拟线程：

CPU密集型任务（计算、排序、加密）
需要精确控制线程的场景
需要线程本地存储的复杂场景
2. 迁移建议
渐进式迁移：

// 第一步：在新功能中使用虚拟线程
@GetMapping("/api/v2/new-endpoint")
public Data newEndpoint() {
    // 使用虚拟线程
    return newService.process();
}

// 第二步：逐步迁移旧代码
// 第三步：完全迁移后，移除线程池配置
AI写代码
java
运行
1
2
3
4
5
6
7
8
9
3. 注意事项
// ❌ 错误：不要创建大量虚拟线程执行CPU任务
try (ExecutorService executor = Executors.newVirtualThreadPerTaskExecutor()) {
    for (int i = 0; i < 1000000; i++) {
        executor.submit(() -> {
            // CPU密集型计算
            heavyComputation();
        });
    }
}

// ✅ 正确：CPU任务用固定线程池
ExecutorService cpuExecutor = Executors.newFixedThreadPool(
    Runtime.getRuntime().availableProcessors()
);
AI写代码
java
运行

1
2
3
4
5
6
7
8
9
10
11
12
13
14
4. 和现有代码兼容
虚拟线程和现有代码完全兼容，不需要改业务逻辑：

// 现有代码
public void existingMethod() {
    // 可以在虚拟线程中运行
    doSomething();
}

// 只需要改变调用方式
// 之前：executor.submit(() -> existingMethod())
// 现在：Thread.ofVirtual().start(() -> existingMethod())
AI写代码
java
运行
1
2
3
4
5
6
7
8
9
总结
虚拟线程确实是个好东西，特别是对IO密集型应用。性能提升虽然不是10倍那么夸张，但3-5倍还是有的。而且代码更简单，不需要考虑线程池配置，用起来很省心。

但也不是万能的，CPU密集型任务还是用传统线程池。关键是要理解适用场景，合理使用。

核心要点：

虚拟线程适合IO密集型任务
代码更简洁，不需要考虑线程池大小
性能提升明显（3-5倍）
与现有代码完全兼容
需要Java 19+（生产环境建议Java 21+）
如果你在做高并发IO应用，可以试试虚拟线程，应该会有惊喜。完整测试代码我放在GitHub上了，需要的同学可以看看。记得给个Star哈哈。
